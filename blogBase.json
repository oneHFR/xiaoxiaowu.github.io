{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "\u5c0f\u5c0f\u5434 \u63d0\u6876\u8dd1\u8def Blog", "subTitle": "check the first blog\ud83d\udc4c: [Original purpose for this blog web]", "avatarUrl": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/Original purpose for this blog web.html", "labels": ["documentation"], "postTitle": "Original purpose for this blog web", "postUrl": "post/Original%20purpose%20for%20this%20blog%20web.html", "postSourceUrl": "https://github.com/oneHFR/xiaoxiaowu.github.io/issues/1", "commentNum": 1, "wordCount": 941, "description": "### BG \ud83e\udd17\ud83d\udc11\r\nHey there~ I am an undergraduate student who has just switched from civil engineering to computer science. I hope to document my gradual growth process! I wish to keep writing daily summaries, continuously reflect on myself, and keep making progress!\r\n\r\nAnd right now it is 2024.09.16 I am in my junior year and literally I only know a little about python and c++ and some other basic theoretical course knowledge (will be forgotten in the short term for sure haha)\r\n\r\nAnd I don't know where I'm going to improve in the next two years so here will be my study diary ~\r\n### Future Plan \ud83d\udcaa\ud83d\udcaa\r\nfollowing years plan maybe \r\n1. take cs degree courses; \r\n2. figure out what academic topics I like and improve my coding skills accordingly; \r\n3. have a satisfying summer research internship\r\n4. I do wish have some academic publications (I know it is a wishful thinking but just a goal)\r\n5. successfully applied for a Ph.D. degree 2026fall;\u3002", "top": 0, "createdAt": 1726489603, "style": "", "script": "", "head": "", "ogImage": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "createdDate": "2024-09-16", "dateLabelColor": "#bc4c00"}, "P2": {"htmlDir": "docs/post/Academic Reading Recording.html", "labels": ["documentation"], "postTitle": "Academic Reading Recording", "postUrl": "post/Academic%20Reading%20Recording.html", "postSourceUrl": "https://github.com/oneHFR/xiaoxiaowu.github.io/issues/2", "commentNum": 0, "wordCount": 2492, "description": "<table>\r\n    <tr>\r\n        <td valign='top' width='500'>\r\n            <p><b>Read Data:</b> 0825</p>\r\n        </td>\r\n        <td valign='top' width='500' colspan='5'>\r\n            <p><b>Publication:</b> CVPR 2021</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td colspan='6' valign='top' width='1000'>\r\n            <b>Title:</b>\r\n            <p>Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing</p>\r\n            <b>Participants:</b>\r\n            <p>Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td colspan='6' valign='top' width='1000'>\r\n            <p><b>Aim:</b></p>\r\n            <p>\u7814\u7a76\u76ee\u6807\u662f\u5728\u4e2d\u95f4\u5c42\u67b6\u6784\u548c\u5b83\u7684\u6548\u7528\u4e4b\u95f4\u67b6\u8d77\u4e00\u5ea7\u6865\u6881\u3002", "top": 0, "createdAt": 1726545718, "style": "", "script": "", "head": "", "ogImage": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "createdDate": "2024-09-17", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/0917 Reading.html", "labels": ["documentation"], "postTitle": "0917 Reading", "postUrl": "post/0917%20Reading.html", "postSourceUrl": "https://github.com/oneHFR/xiaoxiaowu.github.io/issues/3", "commentNum": 1, "wordCount": 11852, "description": "### 0. To Do\r\n\r\n1. \u6570\u5b66\u516c\u5f0f\u7684\u8f93\u5165\u548c\u7406\u89e3  \u27a1\ufe0f  \u8bba\u6587\u6570\u5b66\u8868\u8fbe\u5f0f\u7684\u57fa\u672c\u7406\u89e3\r\n2. \u4ee3\u7801\u6ca1\u6709\u8dd1\u8fc7  \u27a1\ufe0f  Pytorch\u7684\u5165\u95e8\r\n3. \u5728\u7f51\u9875\u4e0a\u7f16\u8f91\u8fd8\u662f\u592a\u96be\u4e86 \u8f6c\u5230\u672c\u5730VScode\u4f1a\u4e0d\u4f1a\u5feb\u6377\u952e\u7f29\u8fdb\u4e4b\u7c7b\u597d\u4e00\u4e9b\uff1f  \u27a1\ufe0f  \u53ca\u65f6\u6e32\u67d3\u7684\u63d2\u4ef6\r\n\r\n### 1. Pick up\r\n\r\n1. **`open-vocabulary detection`** \u27a1\ufe0f The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.\r\n&nbsp;&nbsp;&nbsp;&nbsp;  \u27a1\ufe0f **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // \u5f00\u653e\u8bcd\u6c47\u8868\u76ee\u6807\u68c0\u6d4b \u76ee\u6807\u4e3a\u68c0\u6d4b\u51fa\u5728\u8bad\u7ec3\u4e2d\u6ca1\u6709\u63d0\u4f9b\u6807\u7b7e\u7684\u65b0\u7c7b \u901a\u5e38\u4f34\u968f\u7740\u4efb\u610f\u7684\u6587\u672c\u63cf\u8ff0\r\n2. **`unseen objects`** \u27a1\ufe0f novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.\r\n4. **`vision-language pre-trained models/detector`** \u27a1\ufe0fe.g. [CLIP](https://openai.com/index/clip/)\r\n5. **`2D image pre-trained models`**  \u27a1\ufe0fe.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.\r\n6. **`point-cloud detector`** \u27a1\ufe0f\r\n7. **`image-text pairs`** \u27a1\ufe0f\r\n8. **`semantics`** \u27a1\ufe0f\r\n9. **`embedding layer`** \u27a1\ufe0f\r\n10. **`point-cloud embeddings`** \u27a1\ufe0f\r\n11. **` `** \u27a1\ufe0f\r\n12. **` `** \u27a1\ufe0f\r\n13. **` `** \u27a1\ufe0f\r\n\r\n### 2. Reading table\r\n<table>\r\n    <tr>\r\n        <td valign='top' width='500' colspan='3'>\r\n            <p><b>Read Data:</b> 24.09.17</p>\r\n        </td>\r\n        <td valign='top' width='500' colspan='3'>\r\n            <p><b>Publication:</b> CVPR 2023</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td colspan='6' valign='top' width='1000'>\r\n            <b>Title:</b>\r\n            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation\r\n  <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'>[paper]</a>\r\n  <a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'> [annotation]</a>\r\n<a href = 'https://github.com/lyhdet/OV-3DET'>[code]</a>\r\n</p>\r\n            <b>Participants:</b>\r\n            <p>Yuheng Lu 1\u2217, Chenfeng Xu 2\u2217</p>\r\n</p>\r\n            <b>Dataset:</b>\r\n            <p>\r\n<a href = 'https://paperswithcode.com/dataset/sun-rgb-d'>[SUN RGB]</a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = 'https://paperswithcode.com/dataset/scannet'>[ScanNet</a>\r\n</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td colspan='6' valign='top' width='1000'>\r\n            <p><b>Introduction:</b></p>\r\n            <p>\r\n\r\n1. \u70b9\u4e91\u68c0\u6d4b\u5668\u5728\u6709\u9650\u6570\u91cf\u5bf9\u8c61\u4e0a\u8bad\u7ec3 \u65e0\u6cd5\u6269\u5c55\u5230 \u73b0\u5b9e\u4e30\u5bcc\u7684\u5bf9\u8c61 // Current SOTA point-cloud detectors are trained on a limited classes \u2260 classes in the real world. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;\u21aa\ufe0f\u68c0\u6d4b\u5668\u4e0d\u80fd\u63a8\u5e7f\u770b\u4e0d\u89c1\u5bf9\u8c61 // detectors fail to generalize to   **`unseen object`**   classes <br>\r\n\r\n2. \u5f00\u653e\u8bcd\u6c47\u8868\u68c0\u6d4b\u9700\u8981\u6a21\u578b\u5b66\u4e60\u4e00\u822c\u7684\u8868\u793a\u5e76\u5c06\u5176\u4e0e\u6587\u672c\u8054\u7cfb // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;\u21aa\ufe0f\u70b9\u4e91\u9886\u57df\u6570\u636e\u6536\u96c6\u548c\u6ce8\u91ca\u7684\u56f0\u96be //  the difficulty of both data collection and annotation. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;\u21aa\ufe0f\u963b\u788d\u70b9\u4e91\u68c0\u6d4b\u5668\u5b66\u4f1a\u5982\u4f55\u5c06\u8868\u793a\u4e0e\u6587\u672c\u63d0\u793a\u8fde\u63a5\u8d77\u6765 // hinders point-cloud detectors from learning to connect the representation with text prompts. <br>\r\n&nbsp;\r\n</p>\r\n            <p><b>\ud83d\udca1Aim:</b></p>\r\n            <p>\r\n\r\n1. \u63d0\u51fa **`OV-3DET`** \u5229\u7528**\u56fe\u50cf/\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b**\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u88683D\u70b9\u4e91\u68c0\u6d4b // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection \r\n2. **`OV-3DET`** \u4ee5\u70b9\u4e91\u548c\u6587\u672c\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u6839\u636e\u6587\u672c\u63cf\u8ff0\u68c0\u6d4b\u5bf9\u8c61 \u4e0d\u4f9d\u8d56\u4e8e\u5927\u91cf\u7c7b\u6807\u7b7e\u548c\u6587\u672c\u5bf9\u7684\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e\uff09\r\n<div align='center'>\r\n    <img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig1.png' width='600'>\r\n <em>Fig 1</em>\r\n</div>\r\n&nbsp;\r\n</p>\r\n            <p><b>Research Conclusion:</b></p>\r\n            <p>\r\n\r\n1. \u63d0\u51fa\u5f00\u653e\u8bcd\u6c47\u8868\u7684\u70b9\u4e91\u68c0\u6d4b\u5668 **`OV-3DET`**  <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;  \u2714\ufe0f \u57fa\u4e8e\u4efb\u610f\u7684\u6587\u672c\u63cf\u8ff0\u6765\u672c\u5730\u5316\u548c\u547d\u540d3D\u5bf9\u8c61 // localize and name 3D objects based on arbitrary text descriptions. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;  \u2714\ufe0f **`OV-3DET`** \u7684\u8bad\u7ec3\u4e0d\u9700\u8981\u4efb\u4f553D\u4eba\u5de5\u6ce8\u91ca // not require any 3D human annotations <br>\r\n\r\n2. \u901a\u8fc7**\u4e8c\u7ef4\u9884\u5148\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668**\u548c**\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0** // **`2D image pre-trained detectors`** and **`vision-language models`**. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;  \u2714\ufe0f \u4ece\u4e8c\u7ef4\u9884\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u4e2d\u5b9a\u4f4d\u4e09\u7ef4\u5bf9\u8c61 // localize 3D objects from **`2D pre-trained detectors`**, <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;  \u2714\ufe0f \u901a\u8fc7\u8fde\u63a5\u6587\u672c\u548c\u70b9\u4e91\u5d4c\u5165\u6765\u5bf9\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u8fdb\u884c\u5206\u7c7b // classify the detected objects by connecting text and **`point-cloud embeddings`**. <br>\r\n&nbsp;\r\n</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td valign='top' width='1000' colspan='5'>\r\n            <p><b>Related Work:</b></p>\r\n<p>\r\n\r\n**1. Open-Vocabulary 2D and 3D Detection**\r\n\u3010\u5f85\u603b\u7ed3\u3011\r\n\r\n**2. Weakly-Supervised Detection**\r\n\u3010\u5f85\u603b\u7ed3\u3011\r\n\r\n</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td colspan='6' valign='top' width='1000'>\r\n            <p>\r\n\r\n~~#### Framework Overview:\r\n**`OV-3DET`** is a divide-and conquer method \u27a1\ufe0f two stages\r\n\u2460 **`point-cloud detector`** learns to localize the unknown objects\r\n\u2461 **`point-cloud detector`** learns to name them according to the text prompts.~~\r\n\r\n#### Method:\uff08\u548c\u00b7framework\u7684\u5e8f\u53f7\u987a\u5e8f\u662f\u4e00\u81f4\u7684\uff09\r\n\ud83d\udca1**\u2460 \u3010Localization \u4ece\u4e8c\u7ef4\u9884\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u4e2d\u83b7\u5f97\u5b9a\u4f4d\u80fd\u529b \u6709\u5c55\u5f00\u3011** <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;    \u2714\ufe0f \u76f4\u63a5\u4f7f\u7528\u4e8c\u7ef4\u9884\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u5728\u76f8\u5e94\u7684\u56fe\u50cf\u4e2d\u751f\u6210\u4e00\u7cfb\u5217\u4e8c\u7ef4\u8fb9\u754c\u6846\u6216\u4e8c\u7ef4\u5b9e\u4f8b\u63a9\u7801 // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;    \u2714\ufe0f \u6839\u636e\u70b9\u4e91\u7684\u51e0\u4f55\u5f62\u72b6\u5c06\u68f1\u53f0\uff08\u53d6\u666f\u6846\u770bfig2\uff09\u8f6c\u6362\u4e3a\u76f8\u5bf9\u7d27\u5bc6\u7684\u8fb9\u754c\u76d2\u540e \u2192 \u9884\u6d4b\u7684\u4e8c\u7ef4\u8fb9\u754c\u76d2\u4f5c\u4e3a\u70b9\u4e91\u63a2\u6d4b\u5668\u7684\u4f2a\u8fb9\u754c\u76d2 // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2. \r\n\r\n<div align='center' id='fig8'>\r\n    <img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig2.png' width='500'>\r\n <em>Fig 2</em>\r\n</div>\r\n&nbsp;\r\n\r\n&nbsp;&nbsp;&nbsp;&nbsp;    \u2714\ufe0f \u6ca1\u6709\u4f7f\u7528 class labels predicted by **`2D image pre-trained detector`** <br>\r\n\r\n&nbsp;&nbsp;&nbsp;&nbsp;    \u2714\ufe0f \u4f7f\u7528\u7c97\u7cd9\u7684\u4e8c\u7ef4\u8fb9\u754c\u6846\u6216\u4e8c\u7ef4\u5b9e\u4f8b\u63a9\u7801\u6765\u76d1\u77633D\u70b9\u4e91\u68c0\u6d4b\u5668\u6765\u5b66\u4e60\u5b9a\u4f4d3D\u5bf9\u8c61 // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. <br>\r\n\r\n\r\n\r\n\ud83d\udca1**\u2461 \u3010Classification \u8de8\u6a21\u6001\u5c06\u70b9\u4e91\u5bf9\u8c61\u8fdb\u884c\u5206\u7c7b\u3011** <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;    \u2714\ufe0f \u63d0\u51fa\u4e00\u79cd **\u53bb\u504f\u4e09\u91cd\u6001\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5** \u6765\u5c06\u70b9\u4e91\u3001\u56fe\u50cf\u548c\u6587\u672c\u8054\u7cfb\u8d77\u6765 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;\u2714\ufe0f \u4f7f\u70b9\u4e91\u68c0\u6d4b\u5668\u80fd\u591f\u5c06\u5bf9\u8c61\u4e0e\u76f8\u5e94\u7684\u6587\u672c\u63cf\u8ff0\u8054\u7cfb\u8d77\u6765 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. <br>\r\n&nbsp;&nbsp;&nbsp;&nbsp;\u2714\ufe0f \u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u53ea\u4f7f\u7528\u70b9\u4e91\u68c0\u6d4b\u5668\u548c\u6587\u672c\u63d0\u793a // During inference, only **`point-cloud detector`** and  **`text prompts`** are used. <br>\r\n\r\n&nbsp;\r\n</p>\r\n            <p>&nbsp;\r\n\r\n</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td valign='top' width='1000' colspan='5'>\r\n            <p><b>Results:</b></p>\r\n            <p>&nbsp;\r\n\r\n</p>\r\n            <p>&nbsp;\r\n\r\n</p>\r\n        </td>\r\n    </tr>\r\n    <tr>\r\n        <td valign='top' width='1000' colspan='5'>\r\n            <p><b>Further: (ablation study \u53ea\u7b80\u4ecb\u8bbe\u8ba1\u4e0e\u64cd\u4f5c\u548c\u6240\u5f97\u7ed3\u679c)</b></p>\r\n<p>\r\n\r\n #### ablation\r\n\r\n</p>\r\n        </td>\r\n    </tr>\r\n</table>\r\n\r\n\r\n### 3. Ref-paper\r\n1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)\r\n\r\n2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)\r\n\r\n&nbsp;\r\n\r\n#### 3.2 Notation and Preliminaries\r\n$T$ \u27a1\ufe0f text <br>\r\n$I$ \u27a1\ufe0f image <br>\r\n$P$ \u27a1\ufe0f point-cloud <br>\r\n$I \\in \\mathbb{R}^{3 \\times H \\times W}$, $P = \\{p_i \\in \\mathbb{R}^3, i = 1, 2, 3, \\dots, N\\}$, where $N$ is the point number in the point-cloud. <br>\r\n\r\nDuring training, the unlabeled point-clouds dataset with its paired image is used, denotes as <br>\r\n\r\n$D^{pc}$ = ${{P_j}}^{|D_{pc}|}_{j=1}$ \r\n\r\n$D^{img}$  = ${I_j}^{|D_{img}|}_{j=1}$\r\n\r\n\u3010\u8fd8\u6709\u4e00\u4e9b\u6ca1\u5199\u3011<br>\r\n\r\nPerform open-vocabulary classification by comparing between $f_{1D}$ (text feature) and $f_{3D}$, where $f_{3D}$ represents \r\n\r\n\r\n#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector\r\n1. \u5bf9\u4e8e$D^{pc}$\u548c$D^{img}$\u4e00\u5bf9\u56fe\u50cf\u4e0e\u70b9\u4e91 \u27a1\ufe0f 2D\u9884\u8bad\u7ec3\u63a2\u6d4b\u5668\u9996\u5148\u9884\u6d4b\u4e00\u7cfb\u5217\u76842D\u8fb9\u754c\u6846\u6216\u5b9e\u4f8b\u63a9\u7801 // For a pair of image and point-cloud from $D^{pc}$ and $D^{img}$ \u27a1\ufe0f 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.\r\n\r\n2. \u5c062D\u8fb9\u754c\u6846\u53cd\u5411\u6295\u5f71\u5230\u4e09\u7ef4\u7a7a\u95f4 \u27a1\ufe0f \u5f97\u52303D\u6846 // Back-project the 2D bounding box into 3D space \u27a1\ufe0f the frustum\uff08\u68f1\u53f0\u72b6\uff093D box that could not tightly enclose the 3D object, as shown in [Fig. 2](#fig8)\r\n\r\n3. \u7f29\u5c0f\u4e09\u7ef4\u8fb9\u754c\u6846 \u27a1\ufe0f \u5229\u7528\u70b9\u4e91\u7684\u51e0\u4f55\u5f62\u72b6 \u27a1\ufe0f \u5bf9\u68f1\u53f0\u5185\u7684\u4e09\u7ef4\u70b9\u8fdb\u884c\u805a\u7c7b \u27a1\ufe0f \u53bb\u9664\u80cc\u666f\u70b9\u548c\u79bb\u7fa4\u70b9 // Shrink the 3D bounding box \u27a1\ufe0f leverage the geometry of the point-cloud \u27a1\ufe0f perform clustering on points inside \u27a1\ufe0f remove background and outlier points.\r\n\r\n4. $L_{loc} = L_{box}^{3D}(\\mathbf {\\bar {b}}_{3D}, \\hat {\\mathbf {b}}_{3D})$,   &nbsp;&nbsp;&nbsp;&nbsp; \r\n$\\mathbf {\\bar {b}}_{3D} = cluster(\\mathbf {\\bar {b}}_{2D} \\circ K^{-1})$ <br>\r\n$L_{box}^{3D}$ denotes the bounding box regression\r\nloss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)\r\n\r\n\r\n> - $L_{loc}$ \u8868\u793a\u5c40\u90e8\u635f\u5931\uff0c\u5b83\u4f9d\u8d56\u4e8e\u4e09\u7ef4\u4f2a\u8fb9\u754c\u6846 $L_{box}^{3D}$ \u7684\u8ba1\u7b97\u7ed3\u679c\u3002", "top": 0, "createdAt": 1726636414, "style": "", "script": "<script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "head": "", "ogImage": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "createdDate": "2024-09-18", "dateLabelColor": "#bc4c00"}, "P5": {"htmlDir": "docs/post/Reproduction of OpenScene 1102.html", "labels": ["documentation"], "postTitle": "Reproduction of OpenScene 1102", "postUrl": "post/Reproduction%20of%20OpenScene%201102.html", "postSourceUrl": "https://github.com/oneHFR/xiaoxiaowu.github.io/issues/5", "commentNum": 0, "wordCount": 8030, "description": "# **Reproduction of OpenScene**\r\n\r\n<!-- PROJECT LOGO -->\r\n\r\n<p align='center'>\r\n\r\n  <h1 align='center'><img src='https://pengsongyou.github.io/media/openscene/logo.png' width='40'>OpenScene: 3D Scene Understanding with Open Vocabularies CVPR 2023</h1>\r\n  <h3 align='center'><a href='https://arxiv.org/abs/2211.15654'>Paper</a> | <a href='https://youtu.be/jZxCLHyDJf8'>Video</a> | <a href='https://pengsongyou.github.io/openscene'>Project Page</a></h3>\r\n  <div align='center'></div>\r\n</p>\r\n<p align='center'>\r\n  <a href=''>\r\n    <img src='https://pengsongyou.github.io/media/openscene/teaser.jpg' alt='Logo' width='100%'>\r\n  </a>\r\n</p>\r\n<p align='center'>\r\n<strong>OpenScene</strong> is a zero-shot approach to perform a series of novel 3D scene understanding tasks using open-vocabulary queries.\r\n</p>\r\n<br>\r\n\r\n<!-- TABLE OF CONTENTS -->\r\n<details open='open' style='padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;'>\r\n  <summary>Table of Contents</summary>\r\n  <ol>\r\n    <li>\r\n      <a href='#data-preparation'>Data Preparation</a>\r\n    </li>\r\n    <li>\r\n      <a href='#run'>Run</a>\r\n    </li>\r\n    <li>\r\n      <a href='#others'>Others</a>\r\n    </li>\r\n        <li>\r\n      <a href='#unsolved'>Unsolved</a>\r\n    </li>\r\n    <li>\r\n      <a href='#box'>BOX</a>\r\n    </li>\r\n  </ol>\r\n</details>\r\n\r\n\r\n<span style='font-size:16px; color:gray;'>~~\u4e0b\u9762\u7684\u6307\u4ee4\u5206\u7c7b\u4ee5\u590d\u73b0\u7684README\u6587\u4ef6\u7684\u7f16\u5199\u987a\u5e8f\u5c55\u5f00\uff1f\u5b9e\u9645\u5199\u7684\u65f6\u5019\u662f\u6309\u7167\u65f6\u95f4\u8f74\u5475\u5475~~  </span>\r\n\r\n\u57fa\u672c\u914d\u7f6e\uff1a(\u5f85\u8865\u5145)\r\n\r\n## **Data Preparation**\r\n![\u5c0f\u5434\u4f60\u9700\u8981\u5b66\u4e60\u7684\u8fd8\u6709\u5f88\u591a\uff0cdownload\u4e00\u4e2a\u6570\u636e\u96c6\u5c31\u591f\u4f60\u559d\u4e24\u58f6\u4e86hh](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/TOOL_img/large_scale.png?raw=true)\r\n\r\n### 2D\r\n- [ ] ScanNet 3D (point clouds with GT semantic labels) <span style='font-size:12px; color:gray;'>~~\u4e0b\u8f7d\u4e86\u4e0d\u89c1\u4e86~~</span>\r\n\r\n- [ ] ScanNet 2D (RGB-D images with camera poses) <span style='font-size:12px; color:gray;'>~~\u4e0b\u8f7d\u4e86\u4e0d\u89c1\u4e86 \r\n\u6211\u53d1\u73b0\u4e86 \u662f\u6211\u6628\u5929\u4e0b\u8f7d\u4e4b\u540e\u6ca1\u6709check\u662f\u5426\u89e3\u538b\u6210\u529f \u5c31\u76f4\u63a5\u5220\u6389\u4e86hh \u554a\u554a\u554a\u554a\u554a\u554a\u554a\u554a\u554a...~~</span>\r\n\r\n- [x] Matterport 3D (point clouds with GT semantic labels) \r\n  \u53ea\u6709\u8fd9\u4e00\u4e2a\u6210\u529f\r\n\r\n- [ ] Matterport 2D (RGB-D images with camera poses) <span style='font-size:12px; color:gray;'>\u6ca1\u89e3\u538b\u6210\u529f  \u76f8\u540c\u7684\u95ee\u9898 \u6709\u8d85\u51fa\u6bb5</span>\r\n\r\n### 3D\r\n- [ ] ScanNet - Multi-view fused OpenSeg features, train/val (234.8G) \u8fd8\u5728\u4e0b\u611f\u89c9\u65e0\u671b\r\n\r\n- [ ] Matterport - Multi-view fused OpenSeg features, train/val (198.3G) <span style='font-size:12px; color:gray;'> ~~\u7ed9\u6211\u5220\u4e86\u4e00\u90e8\u5206\u6655\u4e86 </span>\r\n  \r\n- [ ] Matterport - Multi-view fused OpenSeg features, test set (66.7G) \u89e3\u538b\u4e0d\u6b63\u5e38\uff01\uff01\uff01\u62a5\u9519\u67e5\u770b\u4e0b\u6587\r\n\r\n- [x] Replica - Multi-view fused OpenSeg features (9.0G)\r\n\r\n- [ ] nuScenes - Multi-view fused OpenSeg features (coming) \r\n<span style='font-size:12px; color:gray;'>\u597d\u50cf\u6709\u516c\u5f00\u7684\u6570\u636e\u96c6\uff1a \u4f46\u662f\u76ee\u524d\u6765\u770b\u4e0d\u5207\u5b9e\u9645\u56e0\u4e3a\u6570\u636e\u96c6\u592a\u5927 \u5904\u7406\u6570\u636e\u65f6\u95f4\u4e5f\u96be\u4ee5\u4f30\u8ba1 \u5f53\u7136\u8fd8\u662f\u53ef\u4ee5\u7814\u7a76\u4e00\u4e0b\u6570\u636e\u7684pre process\u662f\u5565\u6837\u5b50\u7684\u662f\u600e\u4e48\u4e2a\u4e8b\u60c5\u4e5f\u5f88\u4e0d\u9519\uff01\uff01</span>\r\n\r\n\r\n?\u73b0\u5728\u91cd\u65b0\u7528\u81ea\u5df1\u7684\u811a\u672c\u65ad\u70b9(-c)\u4e0b\u8f7d\u8fd8\u662f\u4f1a\u51fa\u73b0\u4e0b\u8f7d\u6587\u4ef6\u7ed3\u6784\u4e0d\u5b8c\u6574\uff0c\u65e0\u6cd5\u89e3\u538b\u7684\u60c5\u51b5 \u6240\u4ee5remake... \u904d\u5730\u641c\u4e86\u597d\u51e0\u7bc7\u5e16\u5b50\u90fd\u6ca1\u6709\u89e3\u51b3\u51c6\u5907\u7834\u91dc\u6c89\u821fhh\r\n```bash\r\nroot@hostname:~/autodl-tmp/os# unzip matterport_2d.zip\r\nArchive:  matterport_2d.zip\r\n  End-of-central-directory signature not found.  Either this file is not\r\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n  latter case the central directory and zipfile comment will be found on\r\n  the last disk(s) of this archive.\r\nunzip:  cannot find zipfile directory in one of matterport_2d.zip or\r\n        matterport_2d.zip.zip, and cannot find matterport_2d.zip.ZIP, period.\r\n\r\n\r\nroot@hostname:~/autodl-tmp/os/data# unzip scannet_3d.zip\r\nArchive:  scannet_3d.zip\r\nwarning [scannet_3d.zip]:  48103740 extra bytes at beginning or within zipfile\r\n  (attempting to process anyway)\r\nfile #1:  bad zipfile offset (local header sig):  48103740\r\n  (attempting to re-compensate)\r\nerror: invalid zip file with overlapped components (possible zip bomb)\r\n```\r\n\r\n**Trial1\uff1a** \u4e0d\u7528\u811a\u672c\uff1f\u4e0d\u7528\u65ad\u70b9\u4e0b\u8f7d\uff1f \u4e0b\u8f7d\u5230\u522b\u7684\u6587\u4ef6\u5939\uff1f \u6211\u73b0\u5728\u5df2\u7ecf\u5f00\u59cb\u6000\u7591\u662f\u4e0d\u662f\u65ad\u70b9\u7684\u95ee\u9898 \u6bd4\u5982\u5176\u4ed6\u7684 \u6bd4\u5982matterport_3d\u5c31\u662f\u6b63\u5e38\u7684\uff1f ~~\u76f4\u63a5\u4e0b\u8f7d\u5462\uff1f\u611f\u89c9\u8fd9\u4e2a\u53ea\u80fd\u9002\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u5440\uff1f  \r\n**Trial2\uff1a** \u538b\u7f29\u5305transfer\u5230\u672c\u5730   \r\n**Trial3\uff1a** \u8fd8\u6ca1\u5f00\u59cb\u76f4\u63a5\u53bb\u672c\u5730\u4e0b\u8f7d\u518d\u4f20\u8f93\uff1f\u4e0b\u4e0b    \u7b56\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01~~\r\n```bash\r\n# no -c \r\nwget https://cvg-data.inf.ethz.ch/openscene/data/scannet_processed/scannet_2d.zip \r\n```\r\n\r\n\r\n\r\n### Unzip \u89e3\u538b\u73af\u8282\u76ee\u524d\u62a5\u9053\u4e86\u9519\u8bef \u4f46\u662f\u95ee\u9898\u51fa\u73b0\u5728\u4e0a\u4e00\u4e2a\u4e0b\u8f7d\u8fc7\u7a0b\r\n`tar -xvzf` \u548c `tar -xvf` \u533a\u522b\r\n- \u4f7f\u7528 `-z` \u65f6\uff0c\u547d\u4ee4\u9002\u7528\u4e8e `.tar.gz` \u6216 `.tgz` \u6587\u4ef6\u3002", "top": 0, "createdAt": 1730574931, "style": "", "script": "", "head": "", "ogImage": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "createdDate": "2024-11-03", "dateLabelColor": "#bc4c00"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "\u5c0f\u5c0f\u5434 \u63d0\u6876\u8dd1\u8def Blog", "faviconUrl": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "ogImage": "https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://oneHFR.github.io/xiaoxiaowu.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}