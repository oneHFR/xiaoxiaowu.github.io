### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£


### 1. Pick up

1. **`CLIP`** â¡ï¸ match images with their corresponding texts in
open-vocabulary settings
2. **` `** â¡ï¸
3. **` `** â¡ï¸

### 2. Reading table
<table>
    <tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2021</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>PointCLIP: Point Cloud Understanding by CLIP
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf"> [annotation]</a>
<a href = "https://github.com/ZrrSkywalker/PointCLIP">[code1]</a>
<a href = "https://paperswithcode.com/paper/pointclip-point-cloud-understanding-by-clip">[code2]</a>
</p>
            <b>Participants:</b>
            <p>Renrui Zhangâˆ—1, Ziyu Guoâˆ—2</p>
</p>
            <b>Dataset:</b>
            <p> ModelNet10 and ModelNet40 &nbsp;&nbsp;&nbsp;&nbsp;
<a href = "https://paperswithcode.com/dataset/modelnet">[ModelNet_1]</a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = "https://modelnet.cs.princeton.edu/">[ModelNet_2]</a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:(èƒŒæ™¯çŸ¥è¯†/é™åˆ¶)</b></p>
            <p>

1.  ä¸åŸºäºç½‘æ ¼çš„äºŒç»´å›¾åƒæ•°æ®ä¸åŒï¼Œä¸‰ç»´ç‚¹äº‘å­˜åœ¨ç©ºé—´ç¨€ç–æ€§å’Œä¸è§„åˆ™åˆ†å¸ƒçš„é—®é¢˜ï¼Œé˜»ç¢ç›´æ¥ä»äºŒç»´åŸŸçš„è½¬ç§» //<br>
2D image data â¡ï¸ grid-based<br>
3D point clouds â¡ï¸ space sparsity and irregular distribution <br>
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ hinder direct methods transfer from 2D domain <br>
<!-- &nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ //  <br> -->

2.  å¤§è§„æ¨¡çš„æ–°æ•è·ç‚¹äº‘æ•°æ®åŒ…å«å¤§é‡è®­ç»ƒåˆ†ç±»å™¨çš„â€œçœ‹ä¸è§â€ç±»åˆ«å¯¹è±¡ //  large-scale newly captured point cloud data contain a large number of objects of â€œunseenâ€ categories to the trained classifier<br>
<!-- &nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ //  <br> -->


</p>
            <p><b>ğŸ’¡Abstract:</b></p>
            <p>æ–‡ç« åœ¨introéƒ¨åˆ†ä¹Ÿè‡ªå·±æ€»ç»“äº†æœ¬æ–‡è´¡çŒ®4é¡¹ ç¬¬å››é¡¹å°±æ˜¯å¸¸è§„çš„è·‘å®éªŒ

1.  æå‡ºPointCLIP â¡ï¸ å°†ç‚¹äº‘æŠ•å½±åˆ°ä¸éœ€è¦æ¸²æŸ“çš„å¤šè§†å›¾æ·±åº¦å›¾ä¸­æ¥ç¼–ç ç‚¹äº‘ â¡ï¸ å¹¶èšåˆè§†å›¾æ–¹å‘çš„é›¶é•œå¤´é¢„æµ‹ â¡ï¸ å®ç°ä»äºŒç»´åˆ°ä¸‰ç»´çš„çŸ¥è¯†è½¬ç§»ã€‚// propose PointCLIP which conducts alignment between CLIP encoded point cloud and 3D category texts â¡ï¸ encode a point cloud by projecting it into multi-view depth maps without rendering â¡ï¸ aggregate the view-wise zero-shot prediction â¡ï¸ achieve knowledge transfer from 2D to 3D <br>

2.  è®¾è®¡ä¸€ä¸ªè§†å›¾é—´é€‚é…å™¨ â¡ï¸ æ›´å¥½åœ°æå–å…¨å±€ç‰¹å¾ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†ä»3Då­¦ä¹ åˆ°çš„å°‘é•œå¤´çŸ¥è¯†èåˆåˆ°2Dé¢„è®­ç»ƒçš„CLIPä¸­ â¡ï¸ é€šè¿‡åœ¨å°‘é•œå¤´è®¾ç½®ä¸­å¾®è°ƒè½»é‡çº§é€‚é…å™¨ â¡ï¸ æé«˜PointCLIPçš„æ€§èƒ½ // design an inter-view adapter  â¡ï¸ better extract the global feature and adaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in 2D â¡ï¸ by fine-tuning the lightweight adapter in the few-shot settings â¡ï¸ improve the performance of PointCLIP <br>

3.  è§‚å¯Ÿäº†PointCLIPä¸ç»å…¸çš„ä¸‰ç»´ç›‘ç£ç½‘ç»œä¹‹é—´çš„äº’è¡¥æ€§ â¡ï¸ é€šè¿‡ç®€å•çš„é›†æˆï¼ŒPointCLIPæé«˜äº†åŸºçº¿çš„æ€§èƒ½ // observe the complementary property between PointCLIP and classical 3D-supervised networks â¡ï¸ By simple ensembling, PointCLIP boosts baselineâ€™s
performance.<br>

<div align="center">
    <img src="" width="600">
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1.   <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>

2.   <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>

**1. i**
ã€å¾…æ€»ç»“ã€‘

**2. W**
ã€å¾…æ€»ç»“ã€‘

</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
                    <p><b>Method:</b></p>
            <p>

ğŸ’¡**â‘  ã€ã€‘** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  //  <br>



ğŸ’¡**â‘¡ ã€ã€‘** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  //  <br>

&nbsp;
</p>
</td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: </b></p>
<p>

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [å¾…ä¿®æ”¹ï¼](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [å¾…ä¿®æ”¹ï¼](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;



### 4.Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚
- é‡è¦çš„äº‹é¡¹1
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ`è¿™æ˜¯è¡Œå†…ä»£ç `ã€‚
1.*è¿™æ˜¯æ–œä½“æ–‡æœ¬* 2._è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬_
1.**è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬** 2.__è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬__
```python
def hello_world():
    print("Hello, world!")
```
**`open-vocabulary detection`**
[paper](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf)
ğŸ˜ˆ â†’ â–¶ï¸â¡ï¸â†ªï¸â•âœ”ï¸
1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿâ¡ï¸ â—âš ï¸ã€


[Fig. 2](#fig8)
<div align="center" id="fig8">
