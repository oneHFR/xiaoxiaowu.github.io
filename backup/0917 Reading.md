### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶

### 1. Pick up

1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** â¡ï¸
7. **`image-text pairs`** â¡ï¸
8. **`semantics`** â¡ï¸
9. **`embedding layer`** â¡ï¸
10. **`point-cloud embeddings`** â¡ï¸
11. **` `** â¡ï¸
12. **` `** â¡ï¸
13. **` `** â¡ï¸

### 2. Reading table
<table>
    <tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf"> [annotation]</a>
<a href = "https://github.com/lyhdet/OV-3DET">[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = "https://paperswithcode.com/dataset/sun-rgb-d">[SUN RGB]</a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = "https://paperswithcode.com/dataset/scannet">[ScanNet</a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:</b></p>
            <p>

1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world. <br>
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes <br>

2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues. <br>
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation. <br>
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts. <br>
&nbsp;
</p>
            <p><b>ğŸ’¡Aim:</b></p>
            <p>

1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰
<div align="center">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig1.png" width="600">
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`**  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions. <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations <br>

2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**. <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**, <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**. <br>
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
ã€å¾…æ€»ç»“ã€‘

**2. Weakly-Supervised Detection**
ã€å¾…æ€»ç»“ã€‘

</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages
â‘  **`point-cloud detector`** learns to localize the unknown objects
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ› æœ‰å±•å¼€ã€‘** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2. 

<div align="center" id="fig8">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig2.png" width="500">
 <em>Fig 2</em>
</div>
&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`** <br>

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. <br>



ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text <br>
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. <br>
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used. <br>

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

#### 3.2 Notation and Preliminaries
$T$ â¡ï¸ text <br>
$I$ â¡ï¸ image <br>
$P$ â¡ï¸ point-cloud <br>
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. <br>

During training, the unlabeled point-clouds dataset with its paired image is used, denotes as <br>

$D^{pc}$ = ${{P_j}}^{|D_{pc}|}_{j=1}$ 

$D^{img}$  = ${I_j}^{|D_{img}|}_{j=1}$

ã€è¿˜æœ‰ä¸€äº›æ²¡å†™ã€‘<br>

Perform open-vocabulary classification by comparing between $f_{1D}$ (text feature) and $f_{3D}$, where $f_{3D}$ represents 


#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector
1. å¯¹äº$D^{pc}$å’Œ$D^{img}$ä¸€å¯¹å›¾åƒä¸ç‚¹äº‘ â¡ï¸ 2Dé¢„è®­ç»ƒæ¢æµ‹å™¨é¦–å…ˆé¢„æµ‹ä¸€ç³»åˆ—çš„2Dè¾¹ç•Œæ¡†æˆ–å®ä¾‹æ©ç  // For a pair of image and point-cloud from $D^{pc}$ and $D^{img}$ â¡ï¸ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.

2. å°†2Dè¾¹ç•Œæ¡†åå‘æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ â¡ï¸ å¾—åˆ°3Dæ¡† // Back-project the 2D bounding box into 3D space â¡ï¸ the frustumï¼ˆæ£±å°çŠ¶ï¼‰3D box that could not tightly enclose the 3D object, as shown in [Fig. 2](#fig8)

3. ç¼©å°ä¸‰ç»´è¾¹ç•Œæ¡† â¡ï¸ åˆ©ç”¨ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶ â¡ï¸ å¯¹æ£±å°å†…çš„ä¸‰ç»´ç‚¹è¿›è¡Œèšç±» â¡ï¸ å»é™¤èƒŒæ™¯ç‚¹å’Œç¦»ç¾¤ç‚¹ // Shrink the 3D bounding box â¡ï¸ leverage the geometry of the point-cloud â¡ï¸ perform clustering on points inside â¡ï¸ remove background and outlier points.

4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &nbsp;&nbsp;&nbsp;&nbsp; 
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ <br>
$L_{box}^{3D}$ denotes the bounding box regression
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)


> - $L_{loc}$ è¡¨ç¤ºå±€éƒ¨æŸå¤±ï¼Œå®ƒä¾èµ–äºä¸‰ç»´ä¼ªè¾¹ç•Œæ¡† $L_{box}^{3D}$ çš„è®¡ç®—ç»“æœã€‚<br>
> - $L_{box}^{3D}$ æ˜¯ä¸€ä¸ªä¸‰ç»´è¾¹ç•Œæ¡†å›å½’æŸå¤±å‡½æ•°ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªåä¸º3DETRçš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹æ¨¡å‹ <br>
> - $\mathbf{\bar{b}}_{3D}$ æ˜¯ä¸€ä¸ªä¸‰ç»´ä¼ªè¾¹ç•Œæ¡†ï¼Œå®ƒé€šè¿‡å°†äºŒç»´è¾¹ç•Œæ¡† $\mathbf{\bar{b}}_{2D}$ ç»è¿‡é€†å˜æ¢ $K^{-1}$ åï¼Œå†è¿›è¡Œèšç±»å¾—åˆ°ã€‚<br>
> - $\mathbf{\bar{b}}_{2D}$ æ˜¯ä¸€ä¸ªäºŒç»´è¾¹ç•Œæ¡†ï¼Œç”±ä¸€ä¸ªé¢„è®­ç»ƒçš„äºŒç»´æ£€æµ‹å™¨é¢„æµ‹å¾—åˆ°ã€‚<br>
> - $\mathbf{\bar{b}}_{3D}$ åŒ…å«7ä¸ªå‚æ•°ï¼Œç”¨äºè¡¨ç¤ºä¸‰ç»´ç©ºé—´ä¸­çš„ä½ç½®å’Œå°ºå¯¸ä¿¡æ¯ã€‚<br>
> - $\circ$ è¡¨ç¤ºçŸ©é˜µæˆ–å‘é‡çš„ç»„åˆæ“ä½œã€‚<br>
> - $\hat{\mathbf{b}}_{3D}$ æ˜¯çœŸå®ä¸‰ç»´è¾¹ç•Œæ¡†çš„è¡¨ç¤ºï¼Œç”¨äºä¸é¢„æµ‹çš„ $\mathbf{\bar{b}}_{3D}$ è¿›è¡Œæ¯”è¾ƒã€‚<br>

#### 3.4 Learn to Classify 3D Objects from 2D Pre-trained vision-language Model

1. æŒ‡å¯¼æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºä»æœ¬åœ°è¾¹ç•Œæ¡†ä¸­æ‰¾å¯¹åº”ç‰©ä½“ // guide the model to find the objects of interest from localized bounding boxes according to the text prompting.

2. ä»¥å›¾åƒæ¨¡æ€ä¸ºä¸­ä»‹ï¼Œæå‡ºä¸€ç§å»åä¸‰é‡æ€äº¤å‰æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆDTCC æ–‡ä¸­æœ‰è¯¦è§£ æ­¤å¤„ç•¥ï¼‰æ¥è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘ // Take image modality as the intermediary â¡ï¸ a De-biased Triplet Cross Modal Contrastive Learning (DTCC æ–‡ä¸­æœ‰è¯¦è§£ æ­¤å¤„ç•¥) â¡ï¸ connect text and point-cloud

#### 3.5 Synopsis Explain (ä¸»è¦è§£é‡Šè¿™ä¸ªå›¾æ¥ä¸²è®²ä¸Šé¢çš„method)
1. è®­ç»ƒè¿‡ç¨‹ä¸­[**3DETR?**] â¡ï¸ å¾—åˆ°ç‚¹äº‘é¢„æµ‹æ˜¯ä¸€ç³»åˆ—å…·æœ‰$ROI$ç‰¹å¾$f_{3D}$çš„3Dè¾¹ç•Œæ¡† $b^{3D}$

2. åŸºäºæŠ•å½±çŸ©é˜µ$K$æŠ•å½±é¢„æµ‹çš„3Dè¾¹ç•Œæ¡†$b^{3D}$ â¡ï¸ å¯¹ç›¸åº”çš„å›¾åƒè¡¥ä¸è¿›è¡Œè£å‰ªCrop â¡ï¸ å°†å…¶å‘é€åˆ°é¢„å…ˆè®­ç»ƒå¥½çš„è§†è§‰è¯­è¨€æ¨¡å‹[CLIP]ä¸­ â• è¿›è¡Œæ–‡æœ¬æç¤º

3. å¾—åˆ°ï¼šæ–‡æœ¬ç‰¹å¾$f_{1D}$ â• å›¾åƒè¡¥ä¸ç‰¹å¾$f_{2D}$

4. åˆ©ç”¨ **`DTCC`** æ¥è¿æ¥$f_{1D}$ã€$f_{2D}$å’Œ$f_{3D}$



<div align="center">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig3.png" width="700">
 <em>Fig 3</em>
</div>
&nbsp;

<div align="center">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig4.png" width="400">
 <em>Fig 4</em>
</div>
&nbsp;

<div align="center">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig8.png" width="400">
 <em>Fig 8</em>
</div>
&nbsp;


### 4.Scripts {23}
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚
- é‡è¦çš„äº‹é¡¹1
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ`è¿™æ˜¯è¡Œå†…ä»£ç `ã€‚
1.*è¿™æ˜¯æ–œä½“æ–‡æœ¬* 2._è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬_
1.**è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬** 2.__è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬__
```python
def hello_world():
    print("Hello, world!")
```
**`open-vocabulary detection`**
[paper](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf)
ğŸ˜ˆ â†’ â–¶ï¸â¡ï¸â†ªï¸â•âœ”ï¸
1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿâ¡ï¸ â—âš ï¸ã€


[Fig. 2](#fig8)
<div align="center" id="fig8">