### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶

### 1. Pick up

1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** â¡ï¸
7. **`image-text pairs`** â¡ï¸
8. **`semantics`** â¡ï¸
9. **`embedding layer`** â¡ï¸
10. **`point-cloud embeddings`** â¡ï¸
11. **` `** â¡ï¸
12. **` `** â¡ï¸
13. **` `** â¡ï¸

### 2. Reading table
<table>
    <tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf"> [annotation]</a>
<a href = "https://github.com/lyhdet/OV-3DET">[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = "https://paperswithcode.com/dataset/sun-rgb-d">
<b>[SUN RGB]</b></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = "https://paperswithcode.com/dataset/scannet">
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:</b></p>
            <p>

1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts.
&nbsp;
</p>
            <p><b>ğŸ’¡Aim:</b></p>
            <p>

1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰
<div align="center">
    <img src="https://github.com/user-attachments/assets/8e23da74-e5a1-4510-b3f8-3a199a850c4a" width="600">
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`** 
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations

2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**,
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**.
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
ã€å¾…æ€»ç»“ã€‘

**2. Weakly-Supervised Detection**
ã€å¾…æ€»ç»“ã€‘

</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages
â‘  **`point-cloud detector`** learns to localize the unknown objects
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ›ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images. 
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2.

<div align="center">
    <img src="https://github.com/user-attachments/assets/14935857-38d9-4714-962c-9104438bb1e9" width="500">
 <em>Fig 2</em>
</div>
&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects.


ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. 
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used.

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

### 4.Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚
- é‡è¦çš„äº‹é¡¹1
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ`è¿™æ˜¯è¡Œå†…ä»£ç `ã€‚
1.*è¿™æ˜¯æ–œä½“æ–‡æœ¬* 2._è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬_
1.**è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬** 2.__è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬__
```python
def hello_world():
    print("Hello, world!")
```
**`open-vocabulary detection`**
[paper](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf)
ğŸ˜ˆ â†’ â–¶ï¸â¡ï¸â†ªï¸â•âœ”ï¸
1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿâ¡ï¸ â—âš ï¸



