### 1. Pick up

1. **`open-vocabulary detection`** â–¶ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
2. **` `** â–¶ï¸
3. **` `** â–¶ï¸

&nbsp;

### 2. Reading table
<table>
    <tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf"> [annotation]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Aim:</b></p>
            <p>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. å¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ OV-3DET // propose an open-vocabulary point-cloud detector, dubbed OV-3DET, 

> which is capable of localizing and naming 3D objects based on arbitrary text descriptions.
> not require any 3D human **`annotations`**

2. é€šè¿‡äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹å®ç° // achieve this by resorting to **`2D pre-trained detectors`** and **`vision-language models`**.

> localize 3D objects from **`2D pre-trained detectors`**,
> and then classify the detected objects by connecting text and point-cloud **`embeddings`**.

&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Method:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="800" colspan="4">
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
        <td valign="top" width="200" colspan="2">
            <p><b>Discussion:</b></p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="800" colspan="4">
            <p><b>Conclusion:</b></p>
            <p>

&nbsp;

</p>
        </td>
        <td valign="top" width="200">
            <p><b>Further:</b></p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

### 4.Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚
- é‡è¦çš„äº‹é¡¹1
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ`è¿™æ˜¯è¡Œå†…ä»£ç `ã€‚
1.*è¿™æ˜¯æ–œä½“æ–‡æœ¬* 2._è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬_
1.**è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬** 2.__è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬__
```python
def hello_world():
    print("Hello, world!")
```
**`open-vocabulary detection`**
[paper](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf)
ğŸ˜ˆ â†’ â–¶ï¸â¡ï¸â†ªï¸â•âœ”ï¸




