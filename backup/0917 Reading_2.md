### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£


### 1. Pick up

1. **`CLIP`** â¡ï¸ match images with their corresponding texts in
open-vocabulary settings
2. **`Open Vocabulary Learning`** â¡ï¸ [[CSDN]](!https://blog.csdn.net/jiaoyangwm/article/details/132157056) [[OVDç»¼è¿°]](!https://arxiv.org/pdf/2307.09220) [[OVLç»¼è¿°]](!https://arxiv.org/pdf/2306.15880)
3. **`Zero-Shot Learning`** / **`Few-Shot Learning`** â¡ï¸ [[video -IBM]](!https://www.youtube.com/watch?v=pVpr4GYLzAo) è¯¥è§†é¢‘ä¹ŸæåŠåˆ°äº†embedding space<br>
[[Medium-Open-Vocabulary Learning vs. Zero-Shot Learning]](!https://medium.com/@fceydayildiz/open-vocabulary-learning-vs-zero-shot-learning-0e1dce3c5518) Zero-shot learning approaches are designed to learn this **intermediate semantic layer**, the **attributes**, and apply them at inference time to predict new classes, provided with their description in terms of these attributes <br>
ï¼ˆç†è§£æœ‰ç‚¹ç‹­éš˜ï¼Œå±€é™åœ¨åˆçº§çš„detectionâ†’ï¼‰èƒ½ä¸èƒ½ç†è§£ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ„è¯†åˆ°è¿™ä¸ªæ˜¯æ²¡æœ‰è§è¿‡çš„è€Œä¸æ˜¯æŠŠå®ƒå½’ç±»ä¸ºå·²çŸ¥çš„æŸä¸ªlabelä¸Š å¹¶ä¸”ä½¿ä¹‹å¯ä»¥é€šè¿‡æŸä¸ªç¯èŠ‚åˆ†ç±»å‡ºè¿™ä¸ªæ–°ç±» // The objective of zero-shot learning is to enable recognition of â€œunseenâ€ objects which are not adopted during training.

### 2. Reading table
<table>
    <tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2021</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>PointCLIP: Point Cloud Understanding by CLIP
  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf"> [annotation]</a>
<a href = "https://github.com/ZrrSkywalker/PointCLIP">[code1]</a>
<a href = "https://paperswithcode.com/paper/pointclip-point-cloud-understanding-by-clip">[code2]</a>
</p>
            <b>Participants:</b>
            <p>Renrui Zhangâˆ—1, Ziyu Guoâˆ—2</p>
</p>
            <b>Dataset:</b>
            <p> ModelNet10 and ModelNet40 &nbsp;&nbsp;&nbsp;&nbsp;
<a href = "https://paperswithcode.com/dataset/modelnet">[ModelNet_1]</a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = "https://modelnet.cs.princeton.edu/">[ModelNet_2]</a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:(èƒŒæ™¯çŸ¥è¯†/é™åˆ¶)</b></p>
            <p>

1.  ä¸åŸºäºç½‘æ ¼çš„äºŒç»´å›¾åƒæ•°æ®ä¸åŒï¼Œä¸‰ç»´ç‚¹äº‘å­˜åœ¨ç©ºé—´ç¨€ç–æ€§å’Œä¸è§„åˆ™åˆ†å¸ƒçš„é—®é¢˜ï¼Œé˜»ç¢ç›´æ¥ä»äºŒç»´åŸŸçš„è½¬ç§» //<br>
2D image data â¡ï¸ grid-based<br>
3D point clouds â¡ï¸ space sparsity and irregular distribution <br>
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ hinder direct methods transfer from 2D domain <br>
<!-- &nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ //  <br> -->
2.  ModelNet40æœ‰9,843ä¸ªæ ·æœ¬å’Œ40ä¸ªç±» + ImageNetæœ‰100ä¸‡ä¸ªæ ·æœ¬å’Œ1000ä¸ªç±» â¡ï¸ è¿›è¡Œé¢„è®­ç»ƒä¸‰ç»´ç½‘ç»œçš„è¿ç§»å­¦ä¹ æ˜¯éå¸¸å›°éš¾çš„ <br>

3.  å¤§è§„æ¨¡çš„æ–°æ•è·ç‚¹äº‘æ•°æ®åŒ…å«å¤§é‡è®­ç»ƒåˆ†ç±»å™¨çš„â€œçœ‹ä¸è§â€ç±»åˆ«å¯¹è±¡ â¡ï¸ å¦‚æœç”¨ä¼ ç»Ÿçš„ç±»PointNetç½‘ç»œè®­ç»ƒåˆ™é‡åˆ°ä¸€ä¸ªé™Œç”Ÿç‰©ä½“å°±éœ€è¦è®­ç»ƒ //  large-scale newly captured point cloud data contain a large number of objects of â€œunseenâ€ categories to the trained classifier<br>
<!-- &nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ //  <br> -->


</p>
    <p><b>ğŸ’¡Abstract:</b></p>
    <p>æ–‡ç« åœ¨introéƒ¨åˆ†ä¹Ÿè‡ªå·±æ€»ç»“äº†æœ¬æ–‡è´¡çŒ®4é¡¹ ç¬¬å››é¡¹å°±æ˜¯å¸¸è§„çš„è·‘å®éªŒ

1.  æå‡ºPointCLIP â¡ï¸ å°†ç‚¹äº‘æŠ•å½±åˆ°ä¸éœ€è¦æ¸²æŸ“çš„å¤šè§†å›¾æ·±åº¦å›¾ä¸­æ¥ç¼–ç ç‚¹äº‘ â¡ï¸ å¹¶èšåˆè§†å›¾æ–¹å‘çš„é›¶é•œå¤´é¢„æµ‹ â¡ï¸ å®ç°ä»äºŒç»´åˆ°ä¸‰ç»´çš„çŸ¥è¯†è½¬ç§»ã€‚// propose PointCLIP which conducts alignment between CLIP encoded point cloud and 3D category texts â¡ï¸ encode a point cloud by projecting it into multi-view depth maps without rendering â¡ï¸ aggregate the view-wise zero-shot prediction â¡ï¸ achieve knowledge transfer from 2D to 3D <br>

2.  è®¾è®¡ä¸€ä¸ªè§†å›¾é—´é€‚é…å™¨ â¡ï¸ æ›´å¥½åœ°æå–å…¨å±€ç‰¹å¾ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†ä»3Då­¦ä¹ åˆ°çš„å°‘é•œå¤´çŸ¥è¯†èåˆåˆ°2Dé¢„è®­ç»ƒçš„CLIPä¸­ â¡ï¸ é€šè¿‡åœ¨å°‘é•œå¤´è®¾ç½®ä¸­å¾®è°ƒè½»é‡çº§é€‚é…å™¨ â¡ï¸ æé«˜PointCLIPçš„æ€§èƒ½ // design an inter-view adapter  â¡ï¸ better extract the global feature and adaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in 2D â¡ï¸ by fine-tuning the lightweight adapter in the few-shot settings â¡ï¸ improve the performance of PointCLIP <br>

3.  è§‚å¯Ÿäº†PointCLIPä¸ç»å…¸çš„ä¸‰ç»´ç›‘ç£ç½‘ç»œä¹‹é—´çš„äº’è¡¥æ€§ â¡ï¸ é€šè¿‡ç®€å•çš„é›†æˆï¼ŒPointCLIPæé«˜äº†åŸºçº¿çš„æ€§èƒ½ // observe the complementary property between PointCLIP and classical 3D-supervised networks â¡ï¸ By simple ensembling, PointCLIP boosts baselineâ€™s
performance.<br>

<!-- <div align="center">
    <img src="" width="600">
 <em>Fig 2 Pipeline</em>
</div> -->
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1.   <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>

2.   <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸  //  <br>
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>

**1. Zero-shot Learning in 3D** <br>
ã€å¾…æ€»ç»“ã€‘<br>

**2. Transfer Learning** <br>
ã€å¾…æ€»ç»“ã€‘<br>

**3. Deep Neural Networks for Point Cloud** <br>
ã€å¾…æ€»ç»“ã€‘<br>
ã€ä»¥å‰çš„ã€‘<br>
ã€æœ¬æ–‡çš„ã€‘å°†åŸå§‹ç‚¹æŠ•å½±åˆ°å›¾åƒå¹³é¢ä¸Šï¼Œå¹¶æ ¹æ®å‚ç›´è·ç¦»è®¾ç½®å…¶åƒç´ å€¼ï¼Œå¾—åˆ°æ·±åº¦å›¾åƒï¼Œå¯ä»¥å¿½ç•¥ <br>

</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
                    <p><b>Method:</b></p>
            <p>ã€Introéƒ¨åˆ†æœ‰éƒ¨åˆ†æ­¥éª¤ä»‹ç» æˆªå–ä¸€æ®µ çœ‹çœ‹åé¢è¯¦ç»†æè¿°èƒ½ä¸èƒ½å€Ÿæ­¤åŠ æ·±ç†è§£ã€‘ç®€å•åœ°å°†æ¯ä¸ªç‚¹æŠ•å½±åˆ°ä¸€ç³»åˆ—é¢„å®šä¹‰çš„å›¾åƒå¹³é¢ä¸Šï¼Œä»¥ç”Ÿæˆæ•£å°„æ·±åº¦å›¾ // proecting each point onto a series of pre-defifined image planes to generate scatter depth maps

ğŸ’¡**3.1. A Revisit of CLIP** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  CLIPé€šè¿‡è®­ç»ƒå°†å›¾åƒä¸å…¶ç›¸åº”æ–‡æœ¬æè¿°åŒ¹é… // CLIP is trained to match images with their corresponding natural language descriptions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  å…¶ä¸­æœ‰ä¸¤ä¸ªç‹¬ç«‹çš„ç¼–ç å™¨ visual and textual features encoding  â¡ï¸ è®­ç»ƒæ—¶ï¼Œç»™å®šä¸€æ‰¹å›¾åƒå’Œæ–‡æœ¬ï¼ŒCLIPæå–å®ƒä»¬çš„ç‰¹å¾ï¼Œå¹¶å­¦ä¹ åœ¨åµŒå…¥ç©ºé—´ä¸­å¯¹æ¯”å®ƒä»¬ <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  ç¡®ä¿å…¨é¢ï¼Œæ”¶é›†äº†4äº¿å¯¹è®­ç»ƒå›¾åƒæ–‡æœ¬å¯¹ï¼Œä½¿å¾—èƒ½å°†å›¾åƒä¸å¼€æ”¾è¯æ±‡è¡¨ä¸­çš„ä»»ä½•è¯­ä¹‰æ¦‚å¿µå¯¹é½ï¼Œä»¥è¿›è¡ŒOSC <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  è®­ç»ƒè¿‡ç¨‹æ²¡æœ‰æ–°ç…§ç‰‡åŒæ—¶å†»ç»“çš„é¢„è®­ç»ƒç¼–ç å™¨ // The whole process does not require new training images, but achieves promising zero-shot classification performance only by frozen pretrained encoders <br>

<div align="center" id="fig2">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/2-fig2.png" width="700">
 <em><br>Fig 2. Pipeline <br></em>
</div>
&nbsp;

ğŸ’¡**3.2 Point Cloud Understanding by CLIP** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  **â‘  Bridging the Modal Gap** <br>
- å¤šä¸ªè§†è§’æŠ•å½±è·å¾—æ·±åº¦å›¾åƒçš„æ–¹å¼æ¶ˆé™¤æ¨¡æ€gap <br>
- æŠ•å½±æ·±åº¦å›¾æ¥è‡ªåŸå§‹ç‚¹ï¼Œä¸åŒ…å«é¢œè‰²ä¿¡æ¯ï¼Œæ˜¯åˆ†æ•£çš„æ·±åº¦å€¼ï¼Œé™ä½è€—æ—¶å’Œè®¡ç®—æˆæœ¬ <br>
- è¿™æ˜¯ä¸€ç§è½»é‡åŒ–çš„è·¨æ¨¡æ€çš„èåˆ <br>

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  **â‘¡ Zero-shot Classification** <br>
- ã€å¾…æ•´ç†ã€‘<br>
- æ­¤å¤„æ•°å­¦å…¬å¼ç»“åˆä»£ç ç†è§£ <br>


<div align="center" id="fig2">
    <img src="https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/2-fig3.png" width="500">
 <em><br>Fig 3. structure of Inter-view Adapter <br></em>
</div>
&nbsp;

ğŸ’¡**3.3 Inter-view Adapter for PointCLIP** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  æ€§èƒ½è¿˜æ˜¯æ¯”ä¸è¿‡fully-trained 3D neural networksï¼Œæ•…è¿›è¡Œå±€éƒ¨ä¼˜åŒ– <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  å‚è€ƒNLPå’ŒCCLIP-Adapterå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨PointCLIPä¸Šæ·»åŠ äº†ä¸€ä¸ªä¸‰å±‚MLPå³Inter-view Adapterï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶åœ¨Few shotsè®¾ç½®ä¸‹çš„æ€§èƒ½ <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬å†»ç»“äº†CLIPçš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±å¯¹the learnable adapterè¿›è¡Œäº†å¾®è°ƒ <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  The residual style adapter infuses newly-learned 3D few-shot knowledge with that of 2D pre-trained CLIP <br>

ğŸ’¡**3.4. Multi-knowledge Ensembling** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸  ç®€å•é›†æˆåæœ‰å¢å¼ºç†è§£çš„ä½œç”¨ï¼Œå¯å½“æ’ä»¶ <br>

&nbsp;
</p>
</td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Expriment Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: </b></p>
<p>

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [å¾…ä¿®æ”¹ï¼](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [å¾…ä¿®æ”¹ï¼](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;



### 4. Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚
- é‡è¦çš„äº‹é¡¹1
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ`è¿™æ˜¯è¡Œå†…ä»£ç `ã€‚
1.*è¿™æ˜¯æ–œä½“æ–‡æœ¬* 2._è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬_
1.**è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬** 2.__è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬__
```python
def hello_world():
    print("Hello, world!")
```
**`open-vocabulary detection`**
[paper](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf)
ğŸ˜ˆ â†’ â–¶ï¸â¡ï¸â†ªï¸â•âœ”ï¸
1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿâ¡ï¸ â—âš ï¸ã€


[Fig. 2](#fig8)
<div align="center" id="fig8">
