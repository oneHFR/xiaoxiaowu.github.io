<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="### 0. To Do

1. 数学公式的输入和理解  ➡️  论文数学表达式的基本理解
2. 代码没有跑过  ➡️  Pytorch的入门
3. 在网页上编辑还是太难了 转到本地VScode会不会快捷键缩进之类好一些？  ➡️  及时渲染的插件

### 1. Pick up

1. **`open-vocabulary detection`** ➡️ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  ➡️ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // 开放词汇表目标检测 目标为检测出在训练中没有提供标签的新类 通常伴随着任意的文本描述
2. **`unseen objects`** ➡️ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** ➡️e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  ➡️e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** ➡️
7. **`image-text pairs`** ➡️
8. **`semantics`** ➡️
9. **`embedding layer`** ➡️
10. **`point-cloud embeddings`** ➡️
11. **` `** ➡️
12. **` `** ➡️
13. **` `** ➡️

### 2. Reading table
<table>
    <tr>
        <td valign='top' width='500' colspan='3'>
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign='top' width='500' colspan='3'>
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'>[paper]</a>
  <a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'> [annotation]</a>
<a href = 'https://github.com/lyhdet/OV-3DET'>[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1∗, Chenfeng Xu 2∗</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = 'https://paperswithcode.com/dataset/sun-rgb-d'>
<b>[SUN RGB]</b></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = 'https://paperswithcode.com/dataset/scannet'>
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p><b>Introduction:</b></p>
            <p>

1. 点云检测器在有限数量对象上训练 无法扩展到 现实丰富的对象 // Current SOTA point-cloud detectors are trained on a limited classes ≠ classes in the real world.
&nbsp;&nbsp;&nbsp;&nbsp;↪️检测器不能推广看不见对象 // detectors fail to generalize to   **`unseen object`**   classes
2. 开放词汇表检测需要模型学习一般的表示并将其与文本联系 // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.
&nbsp;&nbsp;&nbsp;&nbsp;↪️点云领域数据收集和注释的困难 //  the difficulty of both data collection and annotation.
&nbsp;&nbsp;&nbsp;&nbsp;↪️阻碍点云检测器学会如何将表示与文本提示连接起来 // hinders point-cloud detectors from learning to connect the representation with text prompts.
&nbsp;
</p>
            <p><b>💡Aim:</b></p>
            <p>

1. 提出 **`OV-3DET`** 利用**图像/视觉语言预训练模型**实现开放词汇表3D点云检测 // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** 以点云和文本作为输入，并根据文本描述检测对象 不依赖于大量类标签和文本对的大规模点云数据）
<div align='center'>
    <img src='../OVD_files/img/1-fig1.png' width='600'>
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. 提出开放词汇表的点云检测器 **`OV-3DET`**  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 基于任意的文本描述来本地化和命名3D对象 // localize and name 3D objects based on arbitrary text descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ **`OV-3DET`** 的训练不需要任何3D人工注释 // not require any 3D human annotations

2. 通过**二维预先训练的检测器**和**视觉语言模型实现** // **`2D image pre-trained detectors`** and **`vision-language models`**.
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 从二维预训练的检测器中定位三维对象 // localize 3D objects from **`2D pre-trained detectors`**,
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 通过连接文本和点云嵌入来对检测到的对象进行分类 // classify the detected objects by connecting text and **`point-cloud embeddings`**.
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
【待总结】

**2. Weakly-Supervised Detection**
【待总结】

</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method ➡️ two stages
① **`point-cloud detector`** learns to localize the unknown objects
② **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:（和·framework的序号顺序是一致的）
💡**① 【Localization 从二维预训练的检测器中获得定位能力 有展开】** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 直接使用二维预训练的检测器在相应的图像中生成一系列二维边界框或二维实例掩码 // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 根据点云的几何形状将棱台（取景框看fig2）转换为相对紧密的边界盒后 → 预测的二维边界盒作为点云探测器的伪边界盒 // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2.

<div align='center' id='fig8'>
    <img src='../OVD_files/img/1-fig2.png' width='500'>
 <em>Fig 2</em>
</div>
&nbsp;

<br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 没有使用 class labels predicted by **`2D image pre-trained detector`** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 使用粗糙的二维边界框或二维实例掩码来监督3D点云检测器来学习定位3D对象 // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. <br>


💡**② 【Classification 跨模态将点云对象进行分类】**
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 提出一种 **去偏三重态跨模态对比学习方法** 来将点云、图像和文本联系起来 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. 
💡**② 【Classification 跨模态将点云对象进行分类】** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 提出一种 **去偏三重态跨模态对比学习方法** 来将点云、图像和文本联系起来 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text <br>
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. <br>
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 在推理过程中，只使用点云检测器和文本提示 // During inference, only **`point-cloud detector`** and  **`text prompts`** are used.

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Further: (ablation study 只简介设计与操作和所得结果)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

#### 3.2 Notation and Preliminaries
$T$ ➡️ text <br>
$I$ ➡️ image <br>
$P$ ➡️ point-cloud <br>
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. <br>

During training, the unlabeled point-clouds dataset with its paired image is used, denotes as <br>

$D_{\text{pc}} = \{P_j\}^{|D_{\text{pc}}|}_{j=1}$ 
&nbsp;&nbsp;&nbsp;&nbsp; 
$D_{\text{img}} = \{I_j\}^{|D_{\text{img}}|}_{j=1}$

【还有一些没写】<br>

Perform
open-vocabulary classification by comparing between $f_{1D}$ (text feature)
and $f_{3D}$, where f1D represents 


#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector
1. 对于$D^{pc}$和$D^{img}$一对图像与点云 ➡️ 2D预训练探测器首先预测一系列的2D边界框或实例掩码 // For a pair of image and point-cloud from
$D^{pc}$ and $D^{img}$ ➡️ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.

2. 将2D边界框反向投影到三维空间 ➡️ 得到3D框 // Back-project the 2D bounding box into 3D
space ➡️ the frustum（棱台状）3D box that could not
tightly enclose the 3D object, as shown in [Fig. 2](#fig8)

3. 缩小三维边界框 ➡️ 利用点云的几何形状 ➡️ 对棱台内的三维点进行聚类 ➡️ 去除背景点和离群点 // Shrink the 3D bounding box ➡️ leverage the geometry of the point-cloud ➡️ perform clustering on points inside ➡️ remove background and outlier points.

4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &nbsp;&nbsp;&nbsp;&nbsp; 
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ <br>
$L_{box}^{3D}$ denotes the bounding box regression
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)


> - $L_{loc}$ 表示局部损失，它依赖于三维伪边界框 $L_{box}^{3D}$ 的计算结果。">
<meta property="og:title" content="0917 Reading">
<meta property="og:description" content="### 0. To Do

1. 数学公式的输入和理解  ➡️  论文数学表达式的基本理解
2. 代码没有跑过  ➡️  Pytorch的入门
3. 在网页上编辑还是太难了 转到本地VScode会不会快捷键缩进之类好一些？  ➡️  及时渲染的插件

### 1. Pick up

1. **`open-vocabulary detection`** ➡️ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  ➡️ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // 开放词汇表目标检测 目标为检测出在训练中没有提供标签的新类 通常伴随着任意的文本描述
2. **`unseen objects`** ➡️ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** ➡️e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  ➡️e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** ➡️
7. **`image-text pairs`** ➡️
8. **`semantics`** ➡️
9. **`embedding layer`** ➡️
10. **`point-cloud embeddings`** ➡️
11. **` `** ➡️
12. **` `** ➡️
13. **` `** ➡️

### 2. Reading table
<table>
    <tr>
        <td valign='top' width='500' colspan='3'>
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign='top' width='500' colspan='3'>
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'>[paper]</a>
  <a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'> [annotation]</a>
<a href = 'https://github.com/lyhdet/OV-3DET'>[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1∗, Chenfeng Xu 2∗</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = 'https://paperswithcode.com/dataset/sun-rgb-d'>
<b>[SUN RGB]</b></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = 'https://paperswithcode.com/dataset/scannet'>
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p><b>Introduction:</b></p>
            <p>

1. 点云检测器在有限数量对象上训练 无法扩展到 现实丰富的对象 // Current SOTA point-cloud detectors are trained on a limited classes ≠ classes in the real world.
&nbsp;&nbsp;&nbsp;&nbsp;↪️检测器不能推广看不见对象 // detectors fail to generalize to   **`unseen object`**   classes
2. 开放词汇表检测需要模型学习一般的表示并将其与文本联系 // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.
&nbsp;&nbsp;&nbsp;&nbsp;↪️点云领域数据收集和注释的困难 //  the difficulty of both data collection and annotation.
&nbsp;&nbsp;&nbsp;&nbsp;↪️阻碍点云检测器学会如何将表示与文本提示连接起来 // hinders point-cloud detectors from learning to connect the representation with text prompts.
&nbsp;
</p>
            <p><b>💡Aim:</b></p>
            <p>

1. 提出 **`OV-3DET`** 利用**图像/视觉语言预训练模型**实现开放词汇表3D点云检测 // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** 以点云和文本作为输入，并根据文本描述检测对象 不依赖于大量类标签和文本对的大规模点云数据）
<div align='center'>
    <img src='../OVD_files/img/1-fig1.png' width='600'>
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. 提出开放词汇表的点云检测器 **`OV-3DET`**  <br>
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 基于任意的文本描述来本地化和命名3D对象 // localize and name 3D objects based on arbitrary text descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ **`OV-3DET`** 的训练不需要任何3D人工注释 // not require any 3D human annotations

2. 通过**二维预先训练的检测器**和**视觉语言模型实现** // **`2D image pre-trained detectors`** and **`vision-language models`**.
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 从二维预训练的检测器中定位三维对象 // localize 3D objects from **`2D pre-trained detectors`**,
&nbsp;&nbsp;&nbsp;&nbsp;  ✔️ 通过连接文本和点云嵌入来对检测到的对象进行分类 // classify the detected objects by connecting text and **`point-cloud embeddings`**.
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
【待总结】

**2. Weakly-Supervised Detection**
【待总结】

</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method ➡️ two stages
① **`point-cloud detector`** learns to localize the unknown objects
② **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:（和·framework的序号顺序是一致的）
💡**① 【Localization 从二维预训练的检测器中获得定位能力 有展开】** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 直接使用二维预训练的检测器在相应的图像中生成一系列二维边界框或二维实例掩码 // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 根据点云的几何形状将棱台（取景框看fig2）转换为相对紧密的边界盒后 → 预测的二维边界盒作为点云探测器的伪边界盒 // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2.

<div align='center' id='fig8'>
    <img src='../OVD_files/img/1-fig2.png' width='500'>
 <em>Fig 2</em>
</div>
&nbsp;

<br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 没有使用 class labels predicted by **`2D image pre-trained detector`** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 使用粗糙的二维边界框或二维实例掩码来监督3D点云检测器来学习定位3D对象 // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. <br>


💡**② 【Classification 跨模态将点云对象进行分类】**
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 提出一种 **去偏三重态跨模态对比学习方法** 来将点云、图像和文本联系起来 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. 
💡**② 【Classification 跨模态将点云对象进行分类】** <br>
&nbsp;&nbsp;&nbsp;&nbsp;    ✔️ 提出一种 **去偏三重态跨模态对比学习方法** 来将点云、图像和文本联系起来 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text <br>
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. <br>
&nbsp;&nbsp;&nbsp;&nbsp;✔️ 在推理过程中，只使用点云检测器和文本提示 // During inference, only **`point-cloud detector`** and  **`text prompts`** are used.

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Further: (ablation study 只简介设计与操作和所得结果)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

#### 3.2 Notation and Preliminaries
$T$ ➡️ text <br>
$I$ ➡️ image <br>
$P$ ➡️ point-cloud <br>
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. <br>

During training, the unlabeled point-clouds dataset with its paired image is used, denotes as <br>

$D_{\text{pc}} = \{P_j\}^{|D_{\text{pc}}|}_{j=1}$ 
&nbsp;&nbsp;&nbsp;&nbsp; 
$D_{\text{img}} = \{I_j\}^{|D_{\text{img}}|}_{j=1}$

【还有一些没写】<br>

Perform
open-vocabulary classification by comparing between $f_{1D}$ (text feature)
and $f_{3D}$, where f1D represents 


#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector
1. 对于$D^{pc}$和$D^{img}$一对图像与点云 ➡️ 2D预训练探测器首先预测一系列的2D边界框或实例掩码 // For a pair of image and point-cloud from
$D^{pc}$ and $D^{img}$ ➡️ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.

2. 将2D边界框反向投影到三维空间 ➡️ 得到3D框 // Back-project the 2D bounding box into 3D
space ➡️ the frustum（棱台状）3D box that could not
tightly enclose the 3D object, as shown in [Fig. 2](#fig8)

3. 缩小三维边界框 ➡️ 利用点云的几何形状 ➡️ 对棱台内的三维点进行聚类 ➡️ 去除背景点和离群点 // Shrink the 3D bounding box ➡️ leverage the geometry of the point-cloud ➡️ perform clustering on points inside ➡️ remove background and outlier points.

4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &nbsp;&nbsp;&nbsp;&nbsp; 
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ <br>
$L_{box}^{3D}$ denotes the bounding box regression
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)


> - $L_{loc}$ 表示局部损失，它依赖于三维伪边界框 $L_{box}^{3D}$ 的计算结果。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html">
<meta property="og:image" content="https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg">
<title>0917 Reading</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">0917 Reading</h1>
<div class="title-right">
    <a href="https://oneHFR.github.io/xiaoxiaowu.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/issues/3" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h3>0. To Do</h3>
<ol>
<li>数学公式的输入和理解  ➡️  论文数学表达式的基本理解</li>
<li>代码没有跑过  ➡️  Pytorch的入门</li>
<li>在网页上编辑还是太难了 转到本地VScode会不会快捷键缩进之类好一些？  ➡️  及时渲染的插件</li>
</ol>
<h3>1. Pick up</h3>
<ol>
<li><strong><code class="notranslate">open-vocabulary detection</code></strong> ➡️ The goal of <strong><code class="notranslate">open-vocabulary detection</code></strong> is to identify novel objects based on arbitrary textual descriptions.<br>
      ➡️ <strong><code class="notranslate">open-vocabulary detection</code></strong> (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // 开放词汇表目标检测 目标为检测出在训练中没有提供标签的新类 通常伴随着任意的文本描述</li>
<li><strong><code class="notranslate">unseen objects</code></strong> ➡️ novel objects <strong><code class="notranslate">unseen objects</code></strong> , namely, not ever defined and trained by the already deployed 3D systems.</li>
<li><strong><code class="notranslate">vision-language pre-trained models/detector</code></strong> ➡️e.g. <a href="https://openai.com/index/clip/" rel="nofollow">CLIP</a></li>
<li><strong><code class="notranslate">2D image pre-trained models</code></strong>  ➡️e.g. <a href="https://arxiv.org/pdf/2201.02605" rel="nofollow">Detic</a>   <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" rel="nofollow">Mask R-CNN</a>   <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow">Fast R-CNN</a> etc.</li>
<li><strong><code class="notranslate">point-cloud detector</code></strong> ➡️</li>
<li><strong><code class="notranslate">image-text pairs</code></strong> ➡️</li>
<li><strong><code class="notranslate">semantics</code></strong> ➡️</li>
<li><strong><code class="notranslate">embedding layer</code></strong> ➡️</li>
<li><strong><code class="notranslate">point-cloud embeddings</code></strong> ➡️</li>
<li><strong><code class="notranslate"> </code></strong> ➡️</li>
<li><strong><code class="notranslate"> </code></strong> ➡️</li>
<li><strong><code class="notranslate"> </code></strong> ➡️</li>
</ol>
<h3>2. Reading table</h3>
<markdown-accessiblity-table><table role="table">
    <tbody><tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf" rel="nofollow">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf"> [annotation]</a>
<a href="https://github.com/lyhdet/OV-3DET">[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1∗, Chenfeng Xu 2∗</p>
<p></p>
            <b>Dataset:</b>
            <p>
<a href="https://paperswithcode.com/dataset/sun-rgb-d" rel="nofollow">
<b>[SUN RGB]</b></a>     <a href="https://paperswithcode.com/dataset/scannet" rel="nofollow">
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:</b></p>
            <p>
</p><ol>
<li>点云检测器在有限数量对象上训练 无法扩展到 现实丰富的对象 // Current SOTA point-cloud detectors are trained on a limited classes ≠ classes in the real world.<br>
    ↪️检测器不能推广看不见对象 // detectors fail to generalize to   <strong><code class="notranslate">unseen object</code></strong>   classes</li>
<li>开放词汇表检测需要模型学习一般的表示并将其与文本联系 // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.<br>
    ↪️点云领域数据收集和注释的困难 //  the difficulty of both data collection and annotation.<br>
    ↪️阻碍点云检测器学会如何将表示与文本提示连接起来 // hinders point-cloud detectors from learning to connect the representation with text prompts.<br>
 </li>
</ol>
<p></p>
            <p><b>💡Aim:</b></p>
            <p>
</p><ol>
<li>提出 <strong><code class="notranslate">OV-3DET</code></strong> 利用<strong>图像/视觉语言预训练模型</strong>实现开放词汇表3D点云检测 // propose <strong><code class="notranslate">OV-3DET</code></strong>, which leverages advanced <strong><code class="notranslate">image pre-trained models</code></strong>  and <strong><code class="notranslate">vision-language pre-trained models</code></strong> to achieve <strong>O</strong>pen-<strong>V</strong>ocabulary <strong>3</strong>D point-cloud <strong>DET</strong>ection</li>
<li><strong><code class="notranslate">OV-3DET</code></strong> 以点云和文本作为输入，并根据文本描述检测对象 不依赖于大量类标签和文本对的大规模点云数据）</li>
</ol>
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="../OVD_files/img/1-fig1.png"><img src="../OVD_files/img/1-fig1.png" width="600" style="max-width: 100%;"></a>
 <em>Fig 1</em>
</div>
 
<p></p>
            <p><b>Research Conclusion:</b></p>
            <p>
</p><ol>
<li>
<p>提出开放词汇表的点云检测器 <strong><code class="notranslate">OV-3DET</code></strong>  <br><br>
      ✔️ 基于任意的文本描述来本地化和命名3D对象 // localize and name 3D objects based on arbitrary text descriptions.<br>
      ✔️ <strong><code class="notranslate">OV-3DET</code></strong> 的训练不需要任何3D人工注释 // not require any 3D human annotations</p>
</li>
<li>
<p>通过<strong>二维预先训练的检测器</strong>和<strong>视觉语言模型实现</strong> // <strong><code class="notranslate">2D image pre-trained detectors</code></strong> and <strong><code class="notranslate">vision-language models</code></strong>.<br>
      ✔️ 从二维预训练的检测器中定位三维对象 // localize 3D objects from <strong><code class="notranslate">2D pre-trained detectors</code></strong>,<br>
      ✔️ 通过连接文本和点云嵌入来对检测到的对象进行分类 // classify the detected objects by connecting text and <strong><code class="notranslate">point-cloud embeddings</code></strong>.<br>
 </p>
</li>
</ol>
<p></p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>
</p><p><strong>1. Open-Vocabulary 2D and 3D Detection</strong><br>
【待总结】</p>
<p><strong>2. Weakly-Supervised Detection</strong><br>
【待总结】</p>
<p></p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p>
</p><p><del>#### Framework Overview:<br>
<strong><code class="notranslate">OV-3DET</code></strong> is a divide-and conquer method ➡️ two stages<br>
① <strong><code class="notranslate">point-cloud detector</code></strong> learns to localize the unknown objects<br>
② <strong><code class="notranslate">point-cloud detector</code></strong> learns to name them according to the text prompts.</del></p>
<h4>Method:（和·framework的序号顺序是一致的）</h4>
<p>💡<strong>① 【Localization 从二维预训练的检测器中获得定位能力 有展开】</strong> <br><br>
        ✔️ 直接使用二维预训练的检测器在相应的图像中生成一系列二维边界框或二维实例掩码 // directly take <strong><code class="notranslate">2D image pre-trained detector</code></strong> to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  <br><br>
        ✔️ 根据点云的几何形状将棱台（取景框看fig2）转换为相对紧密的边界盒后 → 预测的二维边界盒作为点云探测器的伪边界盒 // use the <strong><code class="notranslate">predicted 2D bounding boxes</code></strong> as the <strong><code class="notranslate">pseudo bounding box</code></strong> of the point-cloud detector after transforming the <strong><code class="notranslate">frustum</code></strong> into relatively tight bounding box according to the <strong>point-cloud geometry</strong>, as shown in Fig. 2.</p>
<div align="center" id="user-content-fig8">
    <a target="_blank" rel="noopener noreferrer" href="../OVD_files/img/1-fig2.png"><img src="../OVD_files/img/1-fig2.png" width="500" style="max-width: 100%;"></a>
 <em>Fig 2</em>
</div>
 
<br>
        ✔️ 没有使用 class labels predicted by **`2D image pre-trained detector`** <br>
        ✔️ 使用粗糙的二维边界框或二维实例掩码来监督3D点云检测器来学习定位3D对象 // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. <br>
<p>💡<strong>② 【Classification 跨模态将点云对象进行分类】</strong><br>
        ✔️ 提出一种 <strong>去偏三重态跨模态对比学习方法</strong> 来将点云、图像和文本联系起来 // propose a <strong><code class="notranslate">de-biased triplet cross-modal contrastive learning method</code></strong> to connect the modalities among point-cloud, image, and text<br>
    ✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // <strong><code class="notranslate">point-cloud detector</code></strong> is able to relate the objects with corresponding text descriptions.<br>
💡<strong>② 【Classification 跨模态将点云对象进行分类】</strong> <br><br>
        ✔️ 提出一种 <strong>去偏三重态跨模态对比学习方法</strong> 来将点云、图像和文本联系起来 // propose a <strong><code class="notranslate">de-biased triplet cross-modal contrastive learning method</code></strong> to connect the modalities among point-cloud, image, and text <br><br>
    ✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // <strong><code class="notranslate">point-cloud detector</code></strong> is able to relate the objects with corresponding text descriptions. <br><br>
    ✔️ 在推理过程中，只使用点云检测器和文本提示 // During inference, only <strong><code class="notranslate">point-cloud detector</code></strong> and  <strong><code class="notranslate">text prompts</code></strong> are used.</p>
<p> </p>
<p></p>
            <p> 
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Results:</b></p>
            <p> 
</p>
            <p> 
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: (ablation study 只简介设计与操作和所得结果)</b></p>
<p>
</p><h4>ablation</h4>
<p></p>
        </td>
    </tr>
</tbody></table></markdown-accessiblity-table>
<h3>3. Ref-paper</h3>
<ol>
<li>
<p><a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></p>
</li>
<li>
<p><a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a></p>
</li>
</ol>
<p> </p>
<h4>3.2 Notation and Preliminaries</h4>
<p>$T$ ➡️ text <br><br>
$I$ ➡️ image <br><br>
$P$ ➡️ point-cloud <br><br>
$I \in \mathbb{R}^{3 \times H \times W}$, $P = {p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N}$, where $N$ is the point number in the point-cloud. <br></p>
<p>During training, the unlabeled point-clouds dataset with its paired image is used, denotes as <br></p>
<p>$D_{\text{pc}} = {P_j}^{|D_{\text{pc}}|}<em>{j=1}$<br>
    <br>
$D</em>{\text{img}} = {I_j}^{|D_{\text{img}}|}_{j=1}$</p>
<p>【还有一些没写】<br></p>
<p>Perform<br>
open-vocabulary classification by comparing between $f_{1D}$ (text feature)<br>
and $f_{3D}$, where f1D represents</p>
<h4>3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector</h4>
<ol>
<li>
<p>对于$D^{pc}$和$D^{img}$一对图像与点云 ➡️ 2D预训练探测器首先预测一系列的2D边界框或实例掩码 // For a pair of image and point-cloud from<br>
$D^{pc}$ and $D^{img}$ ➡️ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.</p>
</li>
<li>
<p>将2D边界框反向投影到三维空间 ➡️ 得到3D框 // Back-project the 2D bounding box into 3D<br>
space ➡️ the frustum（棱台状）3D box that could not<br>
tightly enclose the 3D object, as shown in <a href="#fig8">Fig. 2</a></p>
</li>
<li>
<p>缩小三维边界框 ➡️ 利用点云的几何形状 ➡️ 对棱台内的三维点进行聚类 ➡️ 去除背景点和离群点 // Shrink the 3D bounding box ➡️ leverage the geometry of the point-cloud ➡️ perform clustering on points inside ➡️ remove background and outlier points.</p>
</li>
<li>
<p>$L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}<em>{3D}, \hat {\mathbf {b}}</em>{3D})$,       <br>
$\mathbf {\bar {b}}<em>{3D} = cluster(\mathbf {\bar {b}}</em>{2D} \circ K^{-1})$ <br><br>
$L_{box}^{3D}$ denotes the bounding box regression<br>
loss used in <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf" rel="nofollow">3DETR</a></p>
</li>
</ol>
<blockquote>
<ul>
<li>
$L_{loc}$ 表示局部损失，它依赖于三维伪边界框 $L_{box}^{3D}$ 的计算结果。<br>
</li>
<li>
$L_{box}^{3D}$ 是一个三维边界框回归损失函数，用于训练一个名为3DETR的三维目标检测模型 <br>
</li>
<li>$\mathbf{\bar{b}}<em>{3D}$ 是一个三维伪边界框，它通过将二维边界框 $\mathbf{\bar{b}}</em>{2D}$ 经过逆变换 $K^{-1}$ 后，再进行聚类得到。<br>
</li>
<li>
$\mathbf{\bar{b}}_{2D}$ 是一个二维边界框，由一个预训练的二维检测器预测得到。<br>
</li>
<li>
$\mathbf{\bar{b}}_{3D}$ 包含7个参数，用于表示三维空间中的位置和尺寸信息。<br>
</li>
<li>
$\circ$ 表示矩阵或向量的组合操作。<br>
</li>
<li>$\hat{\mathbf{b}}<em>{3D}$ 是真实三维边界框的表示，用于与预测的 $\mathbf{\bar{b}}</em>{3D}$ 进行比较。<br>
</li>
</ul>
</blockquote>
<h4>3.4 Learn to Classify 3D Objects from 2D Pre-trained vision-language Model</h4>
<ol>
<li>
<p>指导模型根据文本提示从本地边界框中找对应物体 // guide the model to find the objects of interest from localized bounding boxes according<br>
to the text prompting.</p>
</li>
<li>
<p>以图像模态为中介，提出一种去偏三重态交叉模态对比学习（DTCC 文中有详解 此处略）来连接文本和点云 // Take image modality as<br>
the intermediary ➡️ a De-biased Triplet Cross Modal Contrastive Learning (DTCC 文中有详解 此处略) ➡️ connect text and point-cloud</p>
</li>
</ol>
<h4>3.5 Synopsis Explain (主要解释这个图来串讲上面的method)</h4>
<ol>
<li>
<p>训练过程中[<strong>3DETR?</strong>] ➡️ 得到点云预测是一系列具有$ROI$特征$f_{3D}$的3D边界框 $b^{3D}$</p>
</li>
<li>
<p>基于投影矩阵$K$投影预测的3D边界框$b^{3D}$ ➡️ 对相应的图像补丁进行裁剪Crop ➡️ 将其发送到预先训练好的视觉语言模型[CLIP]中 ➕ 进行文本提示</p>
</li>
<li>
<p>得到：文本特征$f_{1D}$ ➕ 图像补丁特征$f_{2D}$</p>
</li>
<li>
<p>利用 <strong><code class="notranslate">DTCC</code></strong> 来连接$f_{1D}$、$f_{2D}$和$f_{3D}$</p>
</li>
</ol>
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="../OVD_files/img/1-fig3.png"><img src="../OVD_files/img/1-fig3.png" width="700" style="max-width: 100%;"></a>
 <em>Fig 3</em>
</div>
 
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="../OVD_files/img/1-fig4.png"><img src="../OVD_files/img/1-fig4.png" width="400" style="max-width: 100%;"></a>
 <em>Fig 4</em>
</div>
 
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="../OVD_files/img/1-fig8.png"><img src="../OVD_files/img/1-fig8.png" width="400" style="max-width: 100%;"></a>
 <em>Fig 8</em>
</div>
 
<h3>4.Scripts {23}</h3>
<blockquote>
<p>下面是github文本框的markdown强调用法举例？<br>
这是一个引用块，可以用来高亮显示重要信息。</p>
</blockquote>
<ul>
<li>重要的事项1<br>
这是普通文本，而<code class="notranslate">这是行内代码</code>。<br>
1.<em>这是斜体文本</em> 2.<em>这也是斜体文本</em><br>
1.<strong>这是加粗的文本</strong> 2.<strong>这也是加粗的文本</strong></li>
</ul>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">hello_world</span>():
    <span class="pl-en">print</span>(<span class="pl-s">"Hello, world!"</span>)</pre></div>
<p><strong><code class="notranslate">open-vocabulary detection</code></strong><br>
<a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">paper</a><br>
😈 → <g-emoji class="g-emoji" alias="arrow_forward">▶️</g-emoji>➡️↪️➕✔️<br>
1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟➡️ ❗<g-emoji class="g-emoji" alias="warning">⚠️</g-emoji>、</p>
<p><a href="#fig8">Fig. 2</a></p>
<div align="center" id="user-content-fig8"></div></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://oneHFR.github.io/xiaoxiaowu.github.io">小小吴 提桶跑路 Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);
cmButton=document.getElementById("cmButton");
    span=document.createElement("span");
    span.setAttribute("class","Counter");
    span.innerHTML="1";
    cmButton.appendChild(span);


function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","oneHFR/xiaoxiaowu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>
<script>MathJax = {tex: {inlineMath: [["$", "$"]]}};</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</html>
