<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶

### 1. Pick up

1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** â¡ï¸
7. **`image-text pairs`** â¡ï¸
8. **`semantics`** â¡ï¸
9. **`embedding layer`** â¡ï¸
10. **`point-cloud embeddings`** â¡ï¸
11. **` `** â¡ï¸
12. **` `** â¡ï¸
13. **` `** â¡ï¸

### 2. Reading table
<table>
    <tr>
        <td valign='top' width='500' colspan='3'>
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign='top' width='500' colspan='3'>
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'>[paper]</a>
  <a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'> [annotation]</a>
<a href = 'https://github.com/lyhdet/OV-3DET'>[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = 'https://paperswithcode.com/dataset/sun-rgb-d'>
<b>[SUN RGB]</b></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = 'https://paperswithcode.com/dataset/scannet'>
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p><b>Introduction:</b></p>
            <p>

1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts.
&nbsp;
</p>
            <p><b>ğŸ’¡Aim:</b></p>
            <p>

1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰
<div align='center'>
    <img src='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig1.png' width='600'>
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`** 
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations

2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**,
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**.
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
ã€å¾…æ€»ç»“ã€‘

**2. Weakly-Supervised Detection**
ã€å¾…æ€»ç»“ã€‘

</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages
â‘  **`point-cloud detector`** learns to localize the unknown objects
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ›ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images. 
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2.

<div align='center'>
    <img src='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig2.png' width='500'>
 <em>Fig 2</em>
</div>
&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects.


ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. 
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used.

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

### 4.Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚">
<meta property="og:title" content="0917 Reading">
<meta property="og:description" content="### 0. To Do

1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶

### 1. Pick up

1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&nbsp;&nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&nbsp;&nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.
6. **`point-cloud detector`** â¡ï¸
7. **`image-text pairs`** â¡ï¸
8. **`semantics`** â¡ï¸
9. **`embedding layer`** â¡ï¸
10. **`point-cloud embeddings`** â¡ï¸
11. **` `** â¡ï¸
12. **` `** â¡ï¸
13. **` `** â¡ï¸

### 2. Reading table
<table>
    <tr>
        <td valign='top' width='500' colspan='3'>
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign='top' width='500' colspan='3'>
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'>[paper]</a>
  <a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'> [annotation]</a>
<a href = 'https://github.com/lyhdet/OV-3DET'>[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
</p>
            <b>Dataset:</b>
            <p>
<a href = 'https://paperswithcode.com/dataset/sun-rgb-d'>
<b>[SUN RGB]</b></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href = 'https://paperswithcode.com/dataset/scannet'>
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p><b>Introduction:</b></p>
            <p>

1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation.
&nbsp;&nbsp;&nbsp;&nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts.
&nbsp;
</p>
            <p><b>ğŸ’¡Aim:</b></p>
            <p>

1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection 
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰
<div align='center'>
    <img src='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig1.png' width='600'>
 <em>Fig 1</em>
</div>
&nbsp;
</p>
            <p><b>Research Conclusion:</b></p>
            <p>

1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`** 
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations

2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**.
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**,
&nbsp;&nbsp;&nbsp;&nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**.
&nbsp;
</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Related Work:</b></p>
<p>

**1. Open-Vocabulary 2D and 3D Detection**
ã€å¾…æ€»ç»“ã€‘

**2. Weakly-Supervised Detection**
ã€å¾…æ€»ç»“ã€‘

</p>
        </td>
    </tr>
    <tr>
        <td colspan='6' valign='top' width='1000'>
            <p>

~~#### Framework Overview:
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages
â‘  **`point-cloud detector`** learns to localize the unknown objects
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~

#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ›ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images. 
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2.

<div align='center'>
    <img src='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig2.png' width='500'>
 <em>Fig 2</em>
</div>
&nbsp;

&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects.


ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘**
&nbsp;&nbsp;&nbsp;&nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. 
&nbsp;&nbsp;&nbsp;&nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used.

&nbsp;
</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Results:</b></p>
            <p>&nbsp;

</p>
            <p>&nbsp;

</p>
        </td>
    </tr>
    <tr>
        <td valign='top' width='1000' colspan='5'>
            <p><b>Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)</b></p>
<p>

 #### ablation

</p>
        </td>
    </tr>
</table>


### 3. Ref-paper
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)

2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)

&nbsp;

### 4.Scripts
> ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ
>è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚">
<meta property="og:type" content="article">
<meta property="og:url" content="https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html">
<meta property="og:image" content="https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg">
<title>0917 Reading</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">0917 Reading</h1>
<div class="title-right">
    <a href="https://oneHFR.github.io/xiaoxiaowu.github.io" id="buttonHome" class="btn btn-invisible circle" title="é¦–é¡µ">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/issues/3" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="åˆ‡æ¢ä¸»é¢˜">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h3>0. To Do</h3>
<ol>
<li>æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£</li>
<li>ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨</li>
<li>åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶</li>
</ol>
<h3>1. Pick up</h3>
<ol>
<li><strong><code class="notranslate">open-vocabulary detection</code></strong> â¡ï¸ The goal of <strong><code class="notranslate">open-vocabulary detection</code></strong> is to identify novel objects based on arbitrary textual descriptions.<br>
Â Â Â Â   â¡ï¸ <strong><code class="notranslate">open-vocabulary detection</code></strong> (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°</li>
<li><strong><code class="notranslate">unseen objects</code></strong> â¡ï¸ novel objects <strong><code class="notranslate">unseen objects</code></strong> , namely, not ever defined and trained by the already deployed 3D systems.</li>
<li><strong><code class="notranslate">vision-language pre-trained models/detector</code></strong> â¡ï¸e.g. <a href="https://openai.com/index/clip/" rel="nofollow">CLIP</a></li>
<li><strong><code class="notranslate">2D image pre-trained models</code></strong>  â¡ï¸e.g. <a href="https://arxiv.org/pdf/2201.02605" rel="nofollow">Detic</a>Â Â  <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" rel="nofollow">Mask R-CNN</a>Â Â  <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow">Fast R-CNN</a> etc.</li>
<li><strong><code class="notranslate">point-cloud detector</code></strong> â¡ï¸</li>
<li><strong><code class="notranslate">image-text pairs</code></strong> â¡ï¸</li>
<li><strong><code class="notranslate">semantics</code></strong> â¡ï¸</li>
<li><strong><code class="notranslate">embedding layer</code></strong> â¡ï¸</li>
<li><strong><code class="notranslate">point-cloud embeddings</code></strong> â¡ï¸</li>
<li><strong><code class="notranslate"> </code></strong> â¡ï¸</li>
<li><strong><code class="notranslate"> </code></strong> â¡ï¸</li>
<li><strong><code class="notranslate"> </code></strong> â¡ï¸</li>
</ol>
<h3>2. Reading table</h3>
<markdown-accessiblity-table><table role="table">
    <tbody><tr>
        <td valign="top" width="500" colspan="3">
            <p><b>Read Data:</b> 24.09.17</p>
        </td>
        <td valign="top" width="500" colspan="3">
            <p><b>Publication:</b> CVPR 2023</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <b>Title:</b>
            <p>Open-Vocabulary Point-Cloud Object Detection without 3D Annotation
  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf" rel="nofollow">[paper]</a>
  <a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf"> [annotation]</a>
<a href="https://github.com/lyhdet/OV-3DET">[code]</a>
</p>
            <b>Participants:</b>
            <p>Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—</p>
<p></p>
            <b>Dataset:</b>
            <p>
<a href="https://paperswithcode.com/dataset/sun-rgb-d" rel="nofollow">
<b>[SUN RGB]</b></a>Â Â Â Â  <a href="https://paperswithcode.com/dataset/scannet" rel="nofollow">
<b>[ScanNet]</b></a>
</p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p><b>Introduction:</b></p>
            <p>
</p><ol>
<li>ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world.<br>
Â Â Â Â â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   <strong><code class="notranslate">unseen object</code></strong>   classes</li>
<li>å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.<br>
Â Â Â Â â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation.<br>
Â Â Â Â â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts.<br>
Â </li>
</ol>
<p></p>
            <p><b>ğŸ’¡Aim:</b></p>
            <p>
</p><ol>
<li>æå‡º <strong><code class="notranslate">OV-3DET</code></strong> åˆ©ç”¨<strong>å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹</strong>å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose <strong><code class="notranslate">OV-3DET</code></strong>, which leverages advanced <strong><code class="notranslate">image pre-trained models</code></strong>  and <strong><code class="notranslate">vision-language pre-trained models</code></strong> to achieve <strong>O</strong>pen-<strong>V</strong>ocabulary <strong>3</strong>D point-cloud <strong>DET</strong>ection</li>
<li><strong><code class="notranslate">OV-3DET</code></strong> ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰</li>
</ol>
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig1.png"><img src="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig1.png" width="600" style="max-width: 100%;"></a>
 <em>Fig 1</em>
</div>
Â 
<p></p>
            <p><b>Research Conclusion:</b></p>
            <p>
</p><ol>
<li>
<p>æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ <strong><code class="notranslate">OV-3DET</code></strong><br>
Â Â Â Â   âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions.<br>
Â Â Â Â   âœ”ï¸ <strong><code class="notranslate">OV-3DET</code></strong> çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations</p>
</li>
<li>
<p>é€šè¿‡<strong>äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨</strong>å’Œ<strong>è§†è§‰è¯­è¨€æ¨¡å‹å®ç°</strong> // <strong><code class="notranslate">2D image pre-trained detectors</code></strong> and <strong><code class="notranslate">vision-language models</code></strong>.<br>
Â Â Â Â   âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from <strong><code class="notranslate">2D pre-trained detectors</code></strong>,<br>
Â Â Â Â   âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and <strong><code class="notranslate">point-cloud embeddings</code></strong>.<br>
Â </p>
</li>
</ol>
<p></p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Related Work:</b></p>
<p>
</p><p><strong>1. Open-Vocabulary 2D and 3D Detection</strong><br>
ã€å¾…æ€»ç»“ã€‘</p>
<p><strong>2. Weakly-Supervised Detection</strong><br>
ã€å¾…æ€»ç»“ã€‘</p>
<p></p>
        </td>
    </tr>
    <tr>
        <td colspan="6" valign="top" width="1000">
            <p>
</p><p><del>#### Framework Overview:<br>
<strong><code class="notranslate">OV-3DET</code></strong> is a divide-and conquer method â¡ï¸ two stages<br>
â‘  <strong><code class="notranslate">point-cloud detector</code></strong> learns to localize the unknown objects<br>
â‘¡ <strong><code class="notranslate">point-cloud detector</code></strong> learns to name them according to the text prompts.</del></p>
<h4>Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰</h4>
<p>ğŸ’¡<strong>â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ›ã€‘</strong><br>
Â Â Â Â     âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take <strong><code class="notranslate">2D image pre-trained detector</code></strong> to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.<br>
Â Â Â Â     âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the <strong><code class="notranslate">predicted 2D bounding boxes</code></strong> as the <strong><code class="notranslate">pseudo bounding box</code></strong> of the point-cloud detector after transforming the <strong><code class="notranslate">frustum</code></strong> into relatively tight bounding box according to the <strong>point-cloud geometry</strong>, as shown in Fig. 2.</p>
<div align="center">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig2.png"><img src="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/img/1-fig2.png" width="500" style="max-width: 100%;"></a>
 <em>Fig 2</em>
</div>
Â 
<p>Â Â Â Â     âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by <strong><code class="notranslate">2D image pre-trained detector</code></strong><br>
Â Â Â Â     âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise <strong><code class="notranslate">3D point-cloud detectors</code></strong> to learn localizing 3D objects.</p>
<p>ğŸ’¡<strong>â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘</strong><br>
Â Â Â Â     âœ”ï¸ æå‡ºä¸€ç§ <strong>å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•</strong> æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a <strong><code class="notranslate">de-biased triplet cross-modal contrastive learning method</code></strong> to connect the modalities among point-cloud, image, and text<br>
Â Â Â Â âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // <strong><code class="notranslate">point-cloud detector</code></strong> is able to relate the objects with corresponding text descriptions.<br>
Â Â Â Â âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only <strong><code class="notranslate">point-cloud detector</code></strong> and  <strong><code class="notranslate">text prompts</code></strong> are used.</p>
<p>Â </p>
<p></p>
            <p>Â 
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Results:</b></p>
            <p>Â 
</p>
            <p>Â 
</p>
        </td>
    </tr>
    <tr>
        <td valign="top" width="1000" colspan="5">
            <p><b>Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)</b></p>
<p>
</p><h4>ablation</h4>
<p></p>
        </td>
    </tr>
</tbody></table></markdown-accessiblity-table>
<h3>3. Ref-paper</h3>
<ol>
<li>
<p><a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></p>
</li>
<li>
<p><a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a></p>
</li>
</ol>
<p>Â </p>
<h3>4.Scripts</h3>
<blockquote>
<p>ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ<br>
è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚</p>
</blockquote>
<ul>
<li>é‡è¦çš„äº‹é¡¹1<br>
è¿™æ˜¯æ™®é€šæ–‡æœ¬ï¼Œè€Œ<code class="notranslate">è¿™æ˜¯è¡Œå†…ä»£ç </code>ã€‚<br>
1.<em>è¿™æ˜¯æ–œä½“æ–‡æœ¬</em> 2.<em>è¿™ä¹Ÿæ˜¯æ–œä½“æ–‡æœ¬</em><br>
1.<strong>è¿™æ˜¯åŠ ç²—çš„æ–‡æœ¬</strong> 2.<strong>è¿™ä¹Ÿæ˜¯åŠ ç²—çš„æ–‡æœ¬</strong></li>
</ul>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">hello_world</span>():
    <span class="pl-en">print</span>(<span class="pl-s">"Hello, world!"</span>)</pre></div>
<p><strong><code class="notranslate">open-vocabulary detection</code></strong><br>
<a href="https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">paper</a><br>
ğŸ˜ˆ â†’ <g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji>â¡ï¸â†ªï¸â•âœ”ï¸<br>
1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿâ¡ï¸ â—<g-emoji class="g-emoji" alias="warning">âš ï¸</g-emoji></p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">è¯„è®º</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright Â© <span id="copyrightYear"></span> <a href="https://oneHFR.github.io/xiaoxiaowu.github.io">å°å°å´ ææ¡¶è·‘è·¯ Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="ç½‘ç«™è¿è¡Œ"+diffDay+"å¤©"+" â€¢ ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);
cmButton=document.getElementById("cmButton");
    span=document.createElement("span");
    span.setAttribute("class","Counter");
    span.innerHTML="1";
    cmButton.appendChild(span);


function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","oneHFR/xiaoxiaowu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>


</html>
