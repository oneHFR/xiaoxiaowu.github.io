<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>å°å°å´ ææ¡¶è·‘è·¯ Blog</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link><description>check the first blogğŸ‘Œ: [Original purpose for this blog web]</description><copyright>å°å°å´ ææ¡¶è·‘è·¯ Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg</url><title>avatar</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link></image><lastBuildDate>Sat, 02 Nov 2024 19:15:56 +0000</lastBuildDate><managingEditor>å°å°å´ ææ¡¶è·‘è·¯ Blog</managingEditor><ttl>60</ttl><webMaster>å°å°å´ ææ¡¶è·‘è·¯ Blog</webMaster><item><title>Reproduction of OpenScene 1102</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Reproduction%20of%20OpenScene%201102.html</link><description># **Reproduction of OpenScene**&#13;
&#13;
&lt;!-- PROJECT LOGO --&gt;&#13;
&#13;
&lt;p align='center'&gt;&#13;
&#13;
  &lt;h1 align='center'&gt;&lt;img src='https://pengsongyou.github.io/media/openscene/logo.png' width='40'&gt;OpenScene: 3D Scene Understanding with Open Vocabularies CVPR 2023&lt;/h1&gt;&#13;
  &lt;h3 align='center'&gt;&lt;a href='https://arxiv.org/abs/2211.15654'&gt;Paper&lt;/a&gt; | &lt;a href='https://youtu.be/jZxCLHyDJf8'&gt;Video&lt;/a&gt; | &lt;a href='https://pengsongyou.github.io/openscene'&gt;Project Page&lt;/a&gt;&lt;/h3&gt;&#13;
  &lt;div align='center'&gt;&lt;/div&gt;&#13;
&lt;/p&gt;&#13;
&lt;p align='center'&gt;&#13;
  &lt;a href=''&gt;&#13;
    &lt;img src='https://pengsongyou.github.io/media/openscene/teaser.jpg' alt='Logo' width='100%'&gt;&#13;
  &lt;/a&gt;&#13;
&lt;/p&gt;&#13;
&lt;p align='center'&gt;&#13;
&lt;strong&gt;OpenScene&lt;/strong&gt; is a zero-shot approach to perform a series of novel 3D scene understanding tasks using open-vocabulary queries.&#13;
&lt;/p&gt;&#13;
&lt;br&gt;&#13;
&#13;
&lt;!-- TABLE OF CONTENTS --&gt;&#13;
&lt;details open='open' style='padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;'&gt;&#13;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;&#13;
  &lt;ol&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#data-preparation'&gt;Data Preparation&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#run'&gt;Run&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#others'&gt;Others&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
        &lt;li&gt;&#13;
      &lt;a href='#unsolved'&gt;Unsolved&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#box'&gt;BOX&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
  &lt;/ol&gt;&#13;
&lt;/details&gt;&#13;
&#13;
&#13;
&lt;span style='font-size:16px; color:gray;'&gt;~~ä¸‹é¢çš„æŒ‡ä»¤åˆ†ç±»ä»¥å¤ç°çš„READMEæ–‡ä»¶çš„ç¼–å†™é¡ºåºå±•å¼€ï¼Ÿå®é™…å†™çš„æ—¶å€™æ˜¯æŒ‰ç…§æ—¶é—´è½´å‘µå‘µ~~  &lt;/span&gt;&#13;
&#13;
åŸºæœ¬é…ç½®ï¼š(å¾…è¡¥å……)&#13;
&#13;
## **Data Preparation**&#13;
![å°å´ä½ éœ€è¦å­¦ä¹ çš„è¿˜æœ‰å¾ˆå¤šï¼Œdownloadä¸€ä¸ªæ•°æ®é›†å°±å¤Ÿä½ å–ä¸¤å£¶äº†hh](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/TOOL_img/large_scale.png?raw=true)&#13;
&#13;
### 2D&#13;
- [ ] ScanNet 3D (point clouds with GT semantic labels) &lt;span style='font-size:12px; color:gray;'&gt;~~ä¸‹è½½äº†ä¸è§äº†~~&lt;/span&gt;&#13;
&#13;
- [ ] ScanNet 2D (RGB-D images with camera poses) &lt;span style='font-size:12px; color:gray;'&gt;~~ä¸‹è½½äº†ä¸è§äº† &#13;
æˆ‘å‘ç°äº† æ˜¯æˆ‘æ˜¨å¤©ä¸‹è½½ä¹‹åæ²¡æœ‰checkæ˜¯å¦è§£å‹æˆåŠŸ å°±ç›´æ¥åˆ æ‰äº†hh å•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Š...~~&lt;/span&gt;&#13;
&#13;
- [x] Matterport 3D (point clouds with GT semantic labels) &#13;
  åªæœ‰è¿™ä¸€ä¸ªæˆåŠŸ&#13;
&#13;
- [ ] Matterport 2D (RGB-D images with camera poses) &lt;span style='font-size:12px; color:gray;'&gt;æ²¡è§£å‹æˆåŠŸ  ç›¸åŒçš„é—®é¢˜ æœ‰è¶…å‡ºæ®µ&lt;/span&gt;&#13;
&#13;
### 3D&#13;
- [ ] ScanNet - Multi-view fused OpenSeg features, train/val (234.8G) è¿˜åœ¨ä¸‹æ„Ÿè§‰æ— æœ›&#13;
&#13;
- [ ] Matterport - Multi-view fused OpenSeg features, train/val (198.3G) &lt;span style='font-size:12px; color:gray;'&gt; ~~ç»™æˆ‘åˆ äº†ä¸€éƒ¨åˆ†æ™•äº† &lt;/span&gt;&#13;
  &#13;
- [ ] Matterport - Multi-view fused OpenSeg features, test set (66.7G) è§£å‹ä¸æ­£å¸¸ï¼ï¼ï¼æŠ¥é”™æŸ¥çœ‹ä¸‹æ–‡&#13;
&#13;
- [x] Replica - Multi-view fused OpenSeg features (9.0G)&#13;
&#13;
- [ ] nuScenes - Multi-view fused OpenSeg features (coming) &#13;
&lt;span style='font-size:12px; color:gray;'&gt;å¥½åƒæœ‰å…¬å¼€çš„æ•°æ®é›†ï¼š ä½†æ˜¯ç›®å‰æ¥çœ‹ä¸åˆ‡å®é™…å› ä¸ºæ•°æ®é›†å¤ªå¤§ å¤„ç†æ•°æ®æ—¶é—´ä¹Ÿéš¾ä»¥ä¼°è®¡ å½“ç„¶è¿˜æ˜¯å¯ä»¥ç ”ç©¶ä¸€ä¸‹æ•°æ®çš„pre processæ˜¯å•¥æ ·å­çš„æ˜¯æ€ä¹ˆä¸ªäº‹æƒ…ä¹Ÿå¾ˆä¸é”™ï¼ï¼&lt;/span&gt;&#13;
&#13;
&#13;
?ç°åœ¨é‡æ–°ç”¨è‡ªå·±çš„è„šæœ¬æ–­ç‚¹(-c)ä¸‹è½½è¿˜æ˜¯ä¼šå‡ºç°ä¸‹è½½æ–‡ä»¶ç»“æ„ä¸å®Œæ•´ï¼Œæ— æ³•è§£å‹çš„æƒ…å†µ æ‰€ä»¥remake... éåœ°æœäº†å¥½å‡ ç¯‡å¸–å­éƒ½æ²¡æœ‰è§£å†³å‡†å¤‡ç ´é‡œæ²‰èˆŸhh&#13;
```bash&#13;
root@hostname:~/autodl-tmp/os# unzip matterport_2d.zip&#13;
Archive:  matterport_2d.zip&#13;
  End-of-central-directory signature not found.  Either this file is not&#13;
  a zipfile, or it constitutes one disk of a multi-part archive.  In the&#13;
  latter case the central directory and zipfile comment will be found on&#13;
  the last disk(s) of this archive.&#13;
unzip:  cannot find zipfile directory in one of matterport_2d.zip or&#13;
        matterport_2d.zip.zip, and cannot find matterport_2d.zip.ZIP, period.&#13;
&#13;
&#13;
root@hostname:~/autodl-tmp/os/data# unzip scannet_3d.zip&#13;
Archive:  scannet_3d.zip&#13;
warning [scannet_3d.zip]:  48103740 extra bytes at beginning or within zipfile&#13;
  (attempting to process anyway)&#13;
file #1:  bad zipfile offset (local header sig):  48103740&#13;
  (attempting to re-compensate)&#13;
error: invalid zip file with overlapped components (possible zip bomb)&#13;
```&#13;
&#13;
**Trial1ï¼š** ä¸ç”¨è„šæœ¬ï¼Ÿä¸ç”¨æ–­ç‚¹ä¸‹è½½ï¼Ÿ ä¸‹è½½åˆ°åˆ«çš„æ–‡ä»¶å¤¹ï¼Ÿ æˆ‘ç°åœ¨å·²ç»å¼€å§‹æ€€ç–‘æ˜¯ä¸æ˜¯æ–­ç‚¹çš„é—®é¢˜ æ¯”å¦‚å…¶ä»–çš„ æ¯”å¦‚matterport_3då°±æ˜¯æ­£å¸¸çš„ï¼Ÿ ~~ç›´æ¥ä¸‹è½½å‘¢ï¼Ÿæ„Ÿè§‰è¿™ä¸ªåªèƒ½é€‚ç”¨äºå°æ•°æ®é›†å‘€ï¼Ÿ  &#13;
**Trial2ï¼š** å‹ç¼©åŒ…transferåˆ°æœ¬åœ°   &#13;
**Trial3ï¼š** è¿˜æ²¡å¼€å§‹ç›´æ¥å»æœ¬åœ°ä¸‹è½½å†ä¼ è¾“ï¼Ÿä¸‹ä¸‹    ç­–ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼~~&#13;
```bash&#13;
# no -c &#13;
wget https://cvg-data.inf.ethz.ch/openscene/data/scannet_processed/scannet_2d.zip &#13;
```&#13;
&#13;
&#13;
&#13;
### Unzip è§£å‹ç¯èŠ‚ç›®å‰æŠ¥é“äº†é”™è¯¯ ä½†æ˜¯é—®é¢˜å‡ºç°åœ¨ä¸Šä¸€ä¸ªä¸‹è½½è¿‡ç¨‹&#13;
`tar -xvzf` å’Œ `tar -xvf` åŒºåˆ«&#13;
- ä½¿ç”¨ `-z` æ—¶ï¼Œå‘½ä»¤é€‚ç”¨äº `.tar.gz` æˆ– `.tgz` æ–‡ä»¶ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Reproduction%20of%20OpenScene%201102.html</guid><pubDate>Sat, 02 Nov 2024 19:15:31 +0000</pubDate></item><item><title>0917 Reading</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</link><description>### 0. To Do&#13;
&#13;
1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£&#13;
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨&#13;
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶&#13;
&#13;
### 1. Pick up&#13;
&#13;
1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°&#13;
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.&#13;
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)&#13;
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&amp;nbsp;&amp;nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&amp;nbsp;&amp;nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.&#13;
6. **`point-cloud detector`** â¡ï¸&#13;
7. **`image-text pairs`** â¡ï¸&#13;
8. **`semantics`** â¡ï¸&#13;
9. **`embedding layer`** â¡ï¸&#13;
10. **`point-cloud embeddings`** â¡ï¸&#13;
11. **` `** â¡ï¸&#13;
12. **` `** â¡ï¸&#13;
13. **` `** â¡ï¸&#13;
&#13;
### 2. Reading table&#13;
&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 24.09.17&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2023&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Open-Vocabulary Point-Cloud Object Detection without 3D Annotation&#13;
  &lt;a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt;[paper]&lt;/a&gt;&#13;
  &lt;a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt; [annotation]&lt;/a&gt;&#13;
&lt;a href = 'https://github.com/lyhdet/OV-3DET'&gt;[code]&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—&lt;/p&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Dataset:&lt;/b&gt;&#13;
            &lt;p&gt;&#13;
&lt;a href = 'https://paperswithcode.com/dataset/sun-rgb-d'&gt;[SUN RGB]&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href = 'https://paperswithcode.com/dataset/scannet'&gt;[ScanNet&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Introduction:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes &lt;br&gt;&#13;
&#13;
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;ğŸ’¡Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection &#13;
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰&#13;
&lt;div align='center'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig1.png' width='600'&gt;&#13;
 &lt;em&gt;Fig 1&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;Research Conclusion:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`**  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations &lt;br&gt;&#13;
&#13;
2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**, &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Related Work:&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
**1. Open-Vocabulary 2D and 3D Detection**&#13;
ã€å¾…æ€»ç»“ã€‘&#13;
&#13;
**2. Weakly-Supervised Detection**&#13;
ã€å¾…æ€»ç»“ã€‘&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&#13;
&#13;
~~#### Framework Overview:&#13;
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages&#13;
â‘  **`point-cloud detector`** learns to localize the unknown objects&#13;
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~&#13;
&#13;
#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰&#13;
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ› æœ‰å±•å¼€ã€‘** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2. &#13;
&#13;
&lt;div align='center' id='fig8'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig2.png' width='500'&gt;&#13;
 &lt;em&gt;Fig 2&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`** &lt;br&gt;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. &lt;br&gt;&#13;
&#13;
&#13;
&#13;
ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used. &lt;br&gt;&#13;
&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Results:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
 #### ablation&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
&lt;/table&gt;&#13;
&#13;
&#13;
### 3. Ref-paper&#13;
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)&#13;
&#13;
2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)&#13;
&#13;
&amp;nbsp;&#13;
&#13;
#### 3.2 Notation and Preliminaries&#13;
$T$ â¡ï¸ text &lt;br&gt;&#13;
$I$ â¡ï¸ image &lt;br&gt;&#13;
$P$ â¡ï¸ point-cloud &lt;br&gt;&#13;
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. &lt;br&gt;&#13;
&#13;
During training, the unlabeled point-clouds dataset with its paired image is used, denotes as &lt;br&gt;&#13;
&#13;
$D^{pc}$ = ${{P_j}}^{|D_{pc}|}_{j=1}$ &#13;
&#13;
$D^{img}$  = ${I_j}^{|D_{img}|}_{j=1}$&#13;
&#13;
ã€è¿˜æœ‰ä¸€äº›æ²¡å†™ã€‘&lt;br&gt;&#13;
&#13;
Perform open-vocabulary classification by comparing between $f_{1D}$ (text feature) and $f_{3D}$, where $f_{3D}$ represents &#13;
&#13;
&#13;
#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector&#13;
1. å¯¹äº$D^{pc}$å’Œ$D^{img}$ä¸€å¯¹å›¾åƒä¸ç‚¹äº‘ â¡ï¸ 2Dé¢„è®­ç»ƒæ¢æµ‹å™¨é¦–å…ˆé¢„æµ‹ä¸€ç³»åˆ—çš„2Dè¾¹ç•Œæ¡†æˆ–å®ä¾‹æ©ç  // For a pair of image and point-cloud from $D^{pc}$ and $D^{img}$ â¡ï¸ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.&#13;
&#13;
2. å°†2Dè¾¹ç•Œæ¡†åå‘æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ â¡ï¸ å¾—åˆ°3Dæ¡† // Back-project the 2D bounding box into 3D space â¡ï¸ the frustumï¼ˆæ£±å°çŠ¶ï¼‰3D box that could not tightly enclose the 3D object, as shown in [Fig. 2](#fig8)&#13;
&#13;
3. ç¼©å°ä¸‰ç»´è¾¹ç•Œæ¡† â¡ï¸ åˆ©ç”¨ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶ â¡ï¸ å¯¹æ£±å°å†…çš„ä¸‰ç»´ç‚¹è¿›è¡Œèšç±» â¡ï¸ å»é™¤èƒŒæ™¯ç‚¹å’Œç¦»ç¾¤ç‚¹ // Shrink the 3D bounding box â¡ï¸ leverage the geometry of the point-cloud â¡ï¸ perform clustering on points inside â¡ï¸ remove background and outlier points.&#13;
&#13;
4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#13;
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ &lt;br&gt;&#13;
$L_{box}^{3D}$ denotes the bounding box regression&#13;
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)&#13;
&#13;
&#13;
&gt; - $L_{loc}$ è¡¨ç¤ºå±€éƒ¨æŸå¤±ï¼Œå®ƒä¾èµ–äºä¸‰ç»´ä¼ªè¾¹ç•Œæ¡† $L_{box}^{3D}$ çš„è®¡ç®—ç»“æœã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</guid><pubDate>Wed, 18 Sep 2024 05:13:34 +0000</pubDate></item><item><title>Academic Reading Recording</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</link><description>&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 0825&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2021&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;ç ”ç©¶ç›®æ ‡æ˜¯åœ¨ä¸­é—´å±‚æ¶æ„å’Œå®ƒçš„æ•ˆç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</guid><pubDate>Tue, 17 Sep 2024 04:01:58 +0000</pubDate></item><item><title>Original purpose for this blog web</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</link><description>### BG ğŸ¤—ğŸ‘&#13;
Hey there~ I am an undergraduate student who has just switched from civil engineering to computer science. I hope to document my gradual growth process! I wish to keep writing daily summaries, continuously reflect on myself, and keep making progress!&#13;
&#13;
And right now it is 2024.09.16 I am in my junior year and literally I only know a little about python and c++ and some other basic theoretical course knowledge (will be forgotten in the short term for sure haha)&#13;
&#13;
And I don't know where I'm going to improve in the next two years so here will be my study diary ~&#13;
### Future Plan ğŸ’ªğŸ’ª&#13;
following years plan maybe &#13;
1. take cs degree courses; &#13;
2. figure out what academic topics I like and improve my coding skills accordingly; &#13;
3. have a satisfying summer research internship&#13;
4. I do wish have some academic publications (I know it is a wishful thinking but just a goal)&#13;
5. successfully applied for a Ph.D. degree 2026fall;ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</guid><pubDate>Mon, 16 Sep 2024 12:26:43 +0000</pubDate></item></channel></rss>