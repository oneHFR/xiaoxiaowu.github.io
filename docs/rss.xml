<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>å°å°å´ ææ¡¶è·‘è·¯ Blog</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link><description>check the first blogğŸ‘Œ: [Original purpose for this blog web]</description><copyright>å°å°å´ ææ¡¶è·‘è·¯ Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg</url><title>avatar</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link></image><lastBuildDate>Thu, 19 Sep 2024 07:06:02 +0000</lastBuildDate><managingEditor>å°å°å´ ææ¡¶è·‘è·¯ Blog</managingEditor><ttl>60</ttl><webMaster>å°å°å´ ææ¡¶è·‘è·¯ Blog</webMaster><item><title>0917 Reading</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</link><description>### 0. To Do&#13;
&#13;
1. æ•°å­¦å…¬å¼çš„è¾“å…¥å’Œç†è§£  â¡ï¸  è®ºæ–‡æ•°å­¦è¡¨è¾¾å¼çš„åŸºæœ¬ç†è§£&#13;
2. ä»£ç æ²¡æœ‰è·‘è¿‡  â¡ï¸  Pytorchçš„å…¥é—¨&#13;
3. åœ¨ç½‘é¡µä¸Šç¼–è¾‘è¿˜æ˜¯å¤ªéš¾äº† è½¬åˆ°æœ¬åœ°VScodeä¼šä¸ä¼šå¿«æ·é”®ç¼©è¿›ä¹‹ç±»å¥½ä¸€äº›ï¼Ÿ  â¡ï¸  åŠæ—¶æ¸²æŸ“çš„æ’ä»¶&#13;
&#13;
### 1. Pick up&#13;
&#13;
1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  â¡ï¸ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // å¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹ ç›®æ ‡ä¸ºæ£€æµ‹å‡ºåœ¨è®­ç»ƒä¸­æ²¡æœ‰æä¾›æ ‡ç­¾çš„æ–°ç±» é€šå¸¸ä¼´éšç€ä»»æ„çš„æ–‡æœ¬æè¿°&#13;
2. **`unseen objects`** â¡ï¸ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.&#13;
4. **`vision-language pre-trained models/detector`** â¡ï¸e.g. [CLIP](https://openai.com/index/clip/)&#13;
5. **`2D image pre-trained models`**  â¡ï¸e.g. [Detic](https://arxiv.org/pdf/2201.02605)&amp;nbsp;&amp;nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&amp;nbsp;&amp;nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.&#13;
6. **`point-cloud detector`** â¡ï¸&#13;
7. **`image-text pairs`** â¡ï¸&#13;
8. **`semantics`** â¡ï¸&#13;
9. **`embedding layer`** â¡ï¸&#13;
10. **`point-cloud embeddings`** â¡ï¸&#13;
11. **` `** â¡ï¸&#13;
12. **` `** â¡ï¸&#13;
13. **` `** â¡ï¸&#13;
&#13;
### 2. Reading table&#13;
&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 24.09.17&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2023&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Open-Vocabulary Point-Cloud Object Detection without 3D Annotation&#13;
  &lt;a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt;[paper]&lt;/a&gt;&#13;
  &lt;a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt; [annotation]&lt;/a&gt;&#13;
&lt;a href = 'https://github.com/lyhdet/OV-3DET'&gt;[code]&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—&lt;/p&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Dataset:&lt;/b&gt;&#13;
            &lt;p&gt;&#13;
&lt;a href = 'https://paperswithcode.com/dataset/sun-rgb-d'&gt;[SUN RGB]&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href = 'https://paperswithcode.com/dataset/scannet'&gt;[ScanNet&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Introduction:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ æ— æ³•æ‰©å±•åˆ° ç°å®ä¸°å¯Œçš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes &lt;br&gt;&#13;
&#13;
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;ğŸ’¡Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡º **`OV-3DET`** åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection &#13;
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰&#13;
&lt;div align='center'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig1.png' width='600'&gt;&#13;
 &lt;em&gt;Fig 1&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;Research Conclusion:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`**  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations &lt;br&gt;&#13;
&#13;
2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**, &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Related Work:&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
**1. Open-Vocabulary 2D and 3D Detection**&#13;
ã€å¾…æ€»ç»“ã€‘&#13;
&#13;
**2. Weakly-Supervised Detection**&#13;
ã€å¾…æ€»ç»“ã€‘&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&#13;
&#13;
~~#### Framework Overview:&#13;
**`OV-3DET`** is a divide-and conquer method â¡ï¸ two stages&#13;
â‘  **`point-cloud detector`** learns to localize the unknown objects&#13;
â‘¡ **`point-cloud detector`** learns to name them according to the text prompts.~~&#13;
&#13;
#### Method:ï¼ˆå’ŒÂ·frameworkçš„åºå·é¡ºåºæ˜¯ä¸€è‡´çš„ï¼‰&#13;
ğŸ’¡**â‘  ã€Localization ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­è·å¾—å®šä½èƒ½åŠ› æœ‰å±•å¼€ã€‘** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ ç›´æ¥ä½¿ç”¨äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨åœ¨ç›¸åº”çš„å›¾åƒä¸­ç”Ÿæˆä¸€ç³»åˆ—äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç  // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æ ¹æ®ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶å°†æ£±å°ï¼ˆå–æ™¯æ¡†çœ‹fig2ï¼‰è½¬æ¢ä¸ºç›¸å¯¹ç´§å¯†çš„è¾¹ç•Œç›’å â†’ é¢„æµ‹çš„äºŒç»´è¾¹ç•Œç›’ä½œä¸ºç‚¹äº‘æ¢æµ‹å™¨çš„ä¼ªè¾¹ç•Œç›’ // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2. &#13;
&#13;
&lt;div align='center' id='fig8'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig2.png' width='500'&gt;&#13;
 &lt;em&gt;Fig 2&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æ²¡æœ‰ä½¿ç”¨ class labels predicted by **`2D image pre-trained detector`** &lt;br&gt;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ ä½¿ç”¨ç²—ç³™çš„äºŒç»´è¾¹ç•Œæ¡†æˆ–äºŒç»´å®ä¾‹æ©ç æ¥ç›‘ç£3Dç‚¹äº‘æ£€æµ‹å™¨æ¥å­¦ä¹ å®šä½3Då¯¹è±¡ // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. &lt;br&gt;&#13;
&#13;
&#13;
&#13;
ğŸ’¡**â‘¡ ã€Classification è·¨æ¨¡æ€å°†ç‚¹äº‘å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‘** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    âœ”ï¸ æå‡ºä¸€ç§ **å»åä¸‰é‡æ€è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•** æ¥å°†ç‚¹äº‘ã€å›¾åƒå’Œæ–‡æœ¬è”ç³»èµ·æ¥ // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;âœ”ï¸ ä½¿ç‚¹äº‘æ£€æµ‹å™¨èƒ½å¤Ÿå°†å¯¹è±¡ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è”ç³»èµ·æ¥ // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;âœ”ï¸ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨ç‚¹äº‘æ£€æµ‹å™¨å’Œæ–‡æœ¬æç¤º // During inference, only **`point-cloud detector`** and  **`text prompts`** are used. &lt;br&gt;&#13;
&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Results:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Further: (ablation study åªç®€ä»‹è®¾è®¡ä¸æ“ä½œå’Œæ‰€å¾—ç»“æœ)&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
 #### ablation&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
&lt;/table&gt;&#13;
&#13;
&#13;
### 3. Ref-paper&#13;
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)&#13;
&#13;
2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)&#13;
&#13;
&amp;nbsp;&#13;
&#13;
#### 3.2 Notation and Preliminaries&#13;
$T$ â¡ï¸ text &lt;br&gt;&#13;
$I$ â¡ï¸ image &lt;br&gt;&#13;
$P$ â¡ï¸ point-cloud &lt;br&gt;&#13;
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. &lt;br&gt;&#13;
&#13;
During training, the unlabeled point-clouds dataset with its paired image is used, denotes as &lt;br&gt;&#13;
&#13;
$D^{pc}$ = ${{P_j}}^{|D_{pc}|}_{j=1}$ &#13;
&#13;
$D^{img}$  = ${I_j}^{|D_{img}|}_{j=1}$&#13;
&#13;
ã€è¿˜æœ‰ä¸€äº›æ²¡å†™ã€‘&lt;br&gt;&#13;
&#13;
Perform open-vocabulary classification by comparing between $f_{1D}$ (text feature) and $f_{3D}$, where $f_{3D}$ represents &#13;
&#13;
&#13;
#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector&#13;
1. å¯¹äº$D^{pc}$å’Œ$D^{img}$ä¸€å¯¹å›¾åƒä¸ç‚¹äº‘ â¡ï¸ 2Dé¢„è®­ç»ƒæ¢æµ‹å™¨é¦–å…ˆé¢„æµ‹ä¸€ç³»åˆ—çš„2Dè¾¹ç•Œæ¡†æˆ–å®ä¾‹æ©ç  // For a pair of image and point-cloud from $D^{pc}$ and $D^{img}$ â¡ï¸ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.&#13;
&#13;
2. å°†2Dè¾¹ç•Œæ¡†åå‘æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ â¡ï¸ å¾—åˆ°3Dæ¡† // Back-project the 2D bounding box into 3D space â¡ï¸ the frustumï¼ˆæ£±å°çŠ¶ï¼‰3D box that could not tightly enclose the 3D object, as shown in [Fig. 2](#fig8)&#13;
&#13;
3. ç¼©å°ä¸‰ç»´è¾¹ç•Œæ¡† â¡ï¸ åˆ©ç”¨ç‚¹äº‘çš„å‡ ä½•å½¢çŠ¶ â¡ï¸ å¯¹æ£±å°å†…çš„ä¸‰ç»´ç‚¹è¿›è¡Œèšç±» â¡ï¸ å»é™¤èƒŒæ™¯ç‚¹å’Œç¦»ç¾¤ç‚¹ // Shrink the 3D bounding box â¡ï¸ leverage the geometry of the point-cloud â¡ï¸ perform clustering on points inside â¡ï¸ remove background and outlier points.&#13;
&#13;
4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#13;
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ &lt;br&gt;&#13;
$L_{box}^{3D}$ denotes the bounding box regression&#13;
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)&#13;
&#13;
&#13;
&gt; - $L_{loc}$ è¡¨ç¤ºå±€éƒ¨æŸå¤±ï¼Œå®ƒä¾èµ–äºä¸‰ç»´ä¼ªè¾¹ç•Œæ¡† $L_{box}^{3D}$ çš„è®¡ç®—ç»“æœã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</guid><pubDate>Wed, 18 Sep 2024 05:13:34 +0000</pubDate></item><item><title>Academic Reading Recording</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</link><description>&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 0825&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2021&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;ç ”ç©¶ç›®æ ‡æ˜¯åœ¨ä¸­é—´å±‚æ¶æ„å’Œå®ƒçš„æ•ˆç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</guid><pubDate>Tue, 17 Sep 2024 04:01:58 +0000</pubDate></item><item><title>Original purpose for this blog web</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</link><description>### BG ğŸ¤—ğŸ‘&#13;
Hey there~ I am an undergraduate student who has just switched from civil engineering to computer science. I hope to document my gradual growth process! I wish to keep writing daily summaries, continuously reflect on myself, and keep making progress!&#13;
&#13;
And right now it is 2024.09.16 I am in my junior year and literally I only know a little about python and c++ and some other basic theoretical course knowledge (will be forgotten in the short term for sure haha)&#13;
&#13;
And I don't know where I'm going to improve in the next two years so here will be my study diary ~&#13;
### Future Plan ğŸ’ªğŸ’ª&#13;
following years plan maybe &#13;
1. take cs degree courses; &#13;
2. figure out what academic topics I like and improve my coding skills accordingly; &#13;
3. have a satisfying summer research internship&#13;
4. I do wish have some academic publications (I know it is a wishful thinking but just a goal)&#13;
5. successfully applied for a Ph.D. degree 2026fall;ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</guid><pubDate>Mon, 16 Sep 2024 12:26:43 +0000</pubDate></item></channel></rss>