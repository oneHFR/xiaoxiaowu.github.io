<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>å°å°å´ ææ¡¶è·‘è·¯ Blog</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link><description>check the first blogğŸ‘Œ: [Original purpose for this blog web]</description><copyright>å°å°å´ ææ¡¶è·‘è·¯ Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg</url><title>avatar</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link></image><lastBuildDate>Wed, 18 Sep 2024 10:00:20 +0000</lastBuildDate><managingEditor>å°å°å´ ææ¡¶è·‘è·¯ Blog</managingEditor><ttl>60</ttl><webMaster>å°å°å´ ææ¡¶è·‘è·¯ Blog</webMaster><item><title>0917 Reading</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</link><description>### 1. Pick up&#13;
&#13;
1. **`open-vocabulary detection`** â¡ï¸ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.&#13;
2. **`unseen objects`** â¡ï¸ novel objects&#13;
3. **`vision-language pre-trained models`** â¡ï¸e.g. CLIP.&#13;
4. **`image pre-trained models`**  â¡ï¸&#13;
5. **`point-cloud embeddings`** â¡ï¸&#13;
6. **` `** â¡ï¸&#13;
7. **` `** â¡ï¸&#13;
8. **` `** â¡ï¸&#13;
9. **` `** â¡ï¸&#13;
&#13;
### 2. Reading table&#13;
&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 24.09.17&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2023&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Open-Vocabulary Point-Cloud Object Detection without 3D Annotation&#13;
  &lt;a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt;[paper]&lt;/a&gt;&#13;
  &lt;a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt; [annotation]&lt;/a&gt;&#13;
&lt;a href = 'https://github.com/lyhdet/OV-3DET'&gt;[code]&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Yuheng Lu 1âˆ—, Chenfeng Xu 2âˆ—&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Introduction:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. ç‚¹äº‘æ£€æµ‹å™¨åœ¨æœ‰é™æ•°é‡å¯¹è±¡ä¸Šè®­ç»ƒ â‰  è§£é‡Šç°å®çš„å¯¹è±¡ // Current SOTA point-cloud detectors are trained on a limited classes â‰  classes in the real world.&#13;
â†ªï¸æ£€æµ‹å™¨ä¸èƒ½æ¨å¹¿çœ‹ä¸è§å¯¹è±¡ // detectors fail to generalize to   **`unseen object`**   classes&#13;
2. å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹éœ€è¦æ¨¡å‹å­¦ä¹ ä¸€èˆ¬çš„è¡¨ç¤ºå¹¶å°†å…¶ä¸æ–‡æœ¬è”ç³» // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues.&#13;
â†ªï¸ç‚¹äº‘é¢†åŸŸæ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å›°éš¾ //  the difficulty of both data collection and annotation.&#13;
â†ªï¸é˜»ç¢ç‚¹äº‘æ£€æµ‹å™¨å­¦ä¼šå¦‚ä½•å°†è¡¨ç¤ºä¸æ–‡æœ¬æç¤ºè¿æ¥èµ·æ¥ // hinders point-cloud detectors from learning to connect the representation with text prompts.&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;ğŸ’¡Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡ºOV-3DET åˆ©ç”¨**å›¾åƒ/è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹**å®ç°å¼€æ”¾è¯æ±‡è¡¨3Dç‚¹äº‘æ£€æµ‹ // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection &#13;
2. **`OV-3DET`** ä»¥ç‚¹äº‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°æ£€æµ‹å¯¹è±¡ ä¸ä¾èµ–äºå¤§é‡ç±»æ ‡ç­¾å’Œæ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®ï¼‰&#13;
&lt;div align='center'&gt;&#13;
    &lt;img src='https://github.com/user-attachments/assets/8e23da74-e5a1-4510-b3f8-3a199a850c4a' width='600'&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;Research Conclusion:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. æå‡ºå¼€æ”¾è¯æ±‡è¡¨çš„ç‚¹äº‘æ£€æµ‹å™¨ **`OV-3DET`** &#13;
  âœ”ï¸ åŸºäºä»»æ„çš„æ–‡æœ¬æè¿°æ¥æœ¬åœ°åŒ–å’Œå‘½å3Då¯¹è±¡ // localize and name 3D objects based on arbitrary text descriptions.&#13;
  âœ”ï¸ **`OV-3DET`** çš„è®­ç»ƒä¸éœ€è¦ä»»ä½•3Däººå·¥æ³¨é‡Š // not require any 3D human annotations&#13;
&#13;
2. é€šè¿‡**äºŒç»´é¢„å…ˆè®­ç»ƒçš„æ£€æµ‹å™¨**å’Œ**è§†è§‰è¯­è¨€æ¨¡å‹å®ç°** // **`2D image pre-trained detectors`** and **`vision-language models`**.&#13;
  âœ”ï¸ ä»äºŒç»´é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­å®šä½ä¸‰ç»´å¯¹è±¡ // localize 3D objects from **`2D pre-trained detectors`**,&#13;
  âœ”ï¸ é€šè¿‡è¿æ¥æ–‡æœ¬å’Œç‚¹äº‘åµŒå…¥æ¥å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œåˆ†ç±» // classify the detected objects by connecting text and **`point-cloud embeddings`**.&#13;
&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Method:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='800' colspan='4'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Results:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='200' colspan='2'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Discussion:&lt;/b&gt;&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='800' colspan='4'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Conclusion:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='200'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Further:&lt;/b&gt;&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
&lt;/table&gt;&#13;
&#13;
&#13;
### 3. Ref-paper&#13;
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)&#13;
&#13;
2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)&#13;
&#13;
&amp;nbsp;&#13;
&#13;
### 4.Scripts&#13;
&gt; ä¸‹é¢æ˜¯githubæ–‡æœ¬æ¡†çš„markdownå¼ºè°ƒç”¨æ³•ä¸¾ä¾‹ï¼Ÿ&#13;
&gt;è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—ï¼Œå¯ä»¥ç”¨æ¥é«˜äº®æ˜¾ç¤ºé‡è¦ä¿¡æ¯ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</guid><pubDate>Wed, 18 Sep 2024 05:13:34 +0000</pubDate></item><item><title>Academic Reading Recording</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</link><description>&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 0825&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2021&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;ç ”ç©¶ç›®æ ‡æ˜¯åœ¨ä¸­é—´å±‚æ¶æ„å’Œå®ƒçš„æ•ˆç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</guid><pubDate>Tue, 17 Sep 2024 04:01:58 +0000</pubDate></item><item><title>Original purpose for this blog web</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</link><description>### BG ğŸ¤—ğŸ‘&#13;
Hey there~ I am an undergraduate student who has just switched from civil engineering to computer science. I hope to document my gradual growth process! I wish to keep writing daily summaries, continuously reflect on myself, and keep making progress!&#13;
&#13;
And right now it is 2024.09.16 I am in my junior year and literally I only know a little about python and c++ and some other basic theoretical course knowledge (will be forgotten in the short term for sure haha)&#13;
&#13;
And I don't know where I'm going to improve in the next two years so here will be my study diary ~&#13;
### Future Plan ğŸ’ªğŸ’ª&#13;
following years plan maybe &#13;
1. take cs degree courses; &#13;
2. figure out what academic topics I like and improve my coding skills accordingly; &#13;
3. have a satisfying summer research internship&#13;
4. I do wish have some academic publications (I know it is a wishful thinking but just a goal)&#13;
5. successfully applied for a Ph.D. degree 2026fall;ã€‚</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</guid><pubDate>Mon, 16 Sep 2024 12:26:43 +0000</pubDate></item></channel></rss>