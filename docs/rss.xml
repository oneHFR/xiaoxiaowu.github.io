<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>小小吴 提桶跑路 Blog</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link><description>check the first blog👌: [Original purpose for this blog web]</description><copyright>小小吴 提桶跑路 Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://pbs.twimg.com/profile_images/1831494246718959622/uKZLFSPM_400x400.jpg</url><title>avatar</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io</link></image><lastBuildDate>Sat, 02 Nov 2024 19:15:56 +0000</lastBuildDate><managingEditor>小小吴 提桶跑路 Blog</managingEditor><ttl>60</ttl><webMaster>小小吴 提桶跑路 Blog</webMaster><item><title>Reproduction of OpenScene 1102</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Reproduction%20of%20OpenScene%201102.html</link><description># **Reproduction of OpenScene**&#13;
&#13;
&lt;!-- PROJECT LOGO --&gt;&#13;
&#13;
&lt;p align='center'&gt;&#13;
&#13;
  &lt;h1 align='center'&gt;&lt;img src='https://pengsongyou.github.io/media/openscene/logo.png' width='40'&gt;OpenScene: 3D Scene Understanding with Open Vocabularies CVPR 2023&lt;/h1&gt;&#13;
  &lt;h3 align='center'&gt;&lt;a href='https://arxiv.org/abs/2211.15654'&gt;Paper&lt;/a&gt; | &lt;a href='https://youtu.be/jZxCLHyDJf8'&gt;Video&lt;/a&gt; | &lt;a href='https://pengsongyou.github.io/openscene'&gt;Project Page&lt;/a&gt;&lt;/h3&gt;&#13;
  &lt;div align='center'&gt;&lt;/div&gt;&#13;
&lt;/p&gt;&#13;
&lt;p align='center'&gt;&#13;
  &lt;a href=''&gt;&#13;
    &lt;img src='https://pengsongyou.github.io/media/openscene/teaser.jpg' alt='Logo' width='100%'&gt;&#13;
  &lt;/a&gt;&#13;
&lt;/p&gt;&#13;
&lt;p align='center'&gt;&#13;
&lt;strong&gt;OpenScene&lt;/strong&gt; is a zero-shot approach to perform a series of novel 3D scene understanding tasks using open-vocabulary queries.&#13;
&lt;/p&gt;&#13;
&lt;br&gt;&#13;
&#13;
&lt;!-- TABLE OF CONTENTS --&gt;&#13;
&lt;details open='open' style='padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;'&gt;&#13;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;&#13;
  &lt;ol&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#data-preparation'&gt;Data Preparation&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#run'&gt;Run&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#others'&gt;Others&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
        &lt;li&gt;&#13;
      &lt;a href='#unsolved'&gt;Unsolved&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
    &lt;li&gt;&#13;
      &lt;a href='#box'&gt;BOX&lt;/a&gt;&#13;
    &lt;/li&gt;&#13;
  &lt;/ol&gt;&#13;
&lt;/details&gt;&#13;
&#13;
&#13;
&lt;span style='font-size:16px; color:gray;'&gt;~~下面的指令分类以复现的README文件的编写顺序展开？实际写的时候是按照时间轴呵呵~~  &lt;/span&gt;&#13;
&#13;
基本配置：(待补充)&#13;
&#13;
## **Data Preparation**&#13;
![小吴你需要学习的还有很多，download一个数据集就够你喝两壶了hh](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/TOOL_img/large_scale.png?raw=true)&#13;
&#13;
### 2D&#13;
- [ ] ScanNet 3D (point clouds with GT semantic labels) &lt;span style='font-size:12px; color:gray;'&gt;~~下载了不见了~~&lt;/span&gt;&#13;
&#13;
- [ ] ScanNet 2D (RGB-D images with camera poses) &lt;span style='font-size:12px; color:gray;'&gt;~~下载了不见了 &#13;
我发现了 是我昨天下载之后没有check是否解压成功 就直接删掉了hh 啊啊啊啊啊啊啊啊啊...~~&lt;/span&gt;&#13;
&#13;
- [x] Matterport 3D (point clouds with GT semantic labels) &#13;
  只有这一个成功&#13;
&#13;
- [ ] Matterport 2D (RGB-D images with camera poses) &lt;span style='font-size:12px; color:gray;'&gt;没解压成功  相同的问题 有超出段&lt;/span&gt;&#13;
&#13;
### 3D&#13;
- [ ] ScanNet - Multi-view fused OpenSeg features, train/val (234.8G) 还在下感觉无望&#13;
&#13;
- [ ] Matterport - Multi-view fused OpenSeg features, train/val (198.3G) &lt;span style='font-size:12px; color:gray;'&gt; ~~给我删了一部分晕了 &lt;/span&gt;&#13;
  &#13;
- [ ] Matterport - Multi-view fused OpenSeg features, test set (66.7G) 解压不正常！！！报错查看下文&#13;
&#13;
- [x] Replica - Multi-view fused OpenSeg features (9.0G)&#13;
&#13;
- [ ] nuScenes - Multi-view fused OpenSeg features (coming) &#13;
&lt;span style='font-size:12px; color:gray;'&gt;好像有公开的数据集： 但是目前来看不切实际因为数据集太大 处理数据时间也难以估计 当然还是可以研究一下数据的pre process是啥样子的是怎么个事情也很不错！！&lt;/span&gt;&#13;
&#13;
&#13;
?现在重新用自己的脚本断点(-c)下载还是会出现下载文件结构不完整，无法解压的情况 所以remake... 遍地搜了好几篇帖子都没有解决准备破釜沉舟hh&#13;
```bash&#13;
root@hostname:~/autodl-tmp/os# unzip matterport_2d.zip&#13;
Archive:  matterport_2d.zip&#13;
  End-of-central-directory signature not found.  Either this file is not&#13;
  a zipfile, or it constitutes one disk of a multi-part archive.  In the&#13;
  latter case the central directory and zipfile comment will be found on&#13;
  the last disk(s) of this archive.&#13;
unzip:  cannot find zipfile directory in one of matterport_2d.zip or&#13;
        matterport_2d.zip.zip, and cannot find matterport_2d.zip.ZIP, period.&#13;
&#13;
&#13;
root@hostname:~/autodl-tmp/os/data# unzip scannet_3d.zip&#13;
Archive:  scannet_3d.zip&#13;
warning [scannet_3d.zip]:  48103740 extra bytes at beginning or within zipfile&#13;
  (attempting to process anyway)&#13;
file #1:  bad zipfile offset (local header sig):  48103740&#13;
  (attempting to re-compensate)&#13;
error: invalid zip file with overlapped components (possible zip bomb)&#13;
```&#13;
&#13;
**Trial1：** 不用脚本？不用断点下载？ 下载到别的文件夹？ 我现在已经开始怀疑是不是断点的问题 比如其他的 比如matterport_3d就是正常的？ ~~直接下载呢？感觉这个只能适用于小数据集呀？  &#13;
**Trial2：** 压缩包transfer到本地   &#13;
**Trial3：** 还没开始直接去本地下载再传输？下下    策！！！！！！！！~~&#13;
```bash&#13;
# no -c &#13;
wget https://cvg-data.inf.ethz.ch/openscene/data/scannet_processed/scannet_2d.zip &#13;
```&#13;
&#13;
&#13;
&#13;
### Unzip 解压环节目前报道了错误 但是问题出现在上一个下载过程&#13;
`tar -xvzf` 和 `tar -xvf` 区别&#13;
- 使用 `-z` 时，命令适用于 `.tar.gz` 或 `.tgz` 文件。</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Reproduction%20of%20OpenScene%201102.html</guid><pubDate>Sat, 02 Nov 2024 19:15:31 +0000</pubDate></item><item><title>0917 Reading</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</link><description>### 0. To Do&#13;
&#13;
1. 数学公式的输入和理解  ➡️  论文数学表达式的基本理解&#13;
2. 代码没有跑过  ➡️  Pytorch的入门&#13;
3. 在网页上编辑还是太难了 转到本地VScode会不会快捷键缩进之类好一些？  ➡️  及时渲染的插件&#13;
&#13;
### 1. Pick up&#13;
&#13;
1. **`open-vocabulary detection`** ➡️ The goal of **`open-vocabulary detection`** is to identify novel objects based on arbitrary textual descriptions.&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ➡️ **`open-vocabulary detection`** (or known as zero-shot ) targets to detect the novel classes that are not provided labels during training which usually accompanied by arbitrary text description // 开放词汇表目标检测 目标为检测出在训练中没有提供标签的新类 通常伴随着任意的文本描述&#13;
2. **`unseen objects`** ➡️ novel objects **`unseen objects`** , namely, not ever defined and trained by the already deployed 3D systems.&#13;
4. **`vision-language pre-trained models/detector`** ➡️e.g. [CLIP](https://openai.com/index/clip/)&#13;
5. **`2D image pre-trained models`**  ➡️e.g. [Detic](https://arxiv.org/pdf/2201.02605)&amp;nbsp;&amp;nbsp; [Mask R-CNN](https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf)&amp;nbsp;&amp;nbsp; [Fast R-CNN](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) etc.&#13;
6. **`point-cloud detector`** ➡️&#13;
7. **`image-text pairs`** ➡️&#13;
8. **`semantics`** ➡️&#13;
9. **`embedding layer`** ➡️&#13;
10. **`point-cloud embeddings`** ➡️&#13;
11. **` `** ➡️&#13;
12. **` `** ➡️&#13;
13. **` `** ➡️&#13;
&#13;
### 2. Reading table&#13;
&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 24.09.17&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='3'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2023&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Open-Vocabulary Point-Cloud Object Detection without 3D Annotation&#13;
  &lt;a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt;[paper]&lt;/a&gt;&#13;
  &lt;a href='https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf'&gt; [annotation]&lt;/a&gt;&#13;
&lt;a href = 'https://github.com/lyhdet/OV-3DET'&gt;[code]&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Yuheng Lu 1∗, Chenfeng Xu 2∗&lt;/p&gt;&#13;
&lt;/p&gt;&#13;
            &lt;b&gt;Dataset:&lt;/b&gt;&#13;
            &lt;p&gt;&#13;
&lt;a href = 'https://paperswithcode.com/dataset/sun-rgb-d'&gt;[SUN RGB]&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href = 'https://paperswithcode.com/dataset/scannet'&gt;[ScanNet&lt;/a&gt;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Introduction:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. 点云检测器在有限数量对象上训练 无法扩展到 现实丰富的对象 // Current SOTA point-cloud detectors are trained on a limited classes ≠ classes in the real world. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;↪️检测器不能推广看不见对象 // detectors fail to generalize to   **`unseen object`**   classes &lt;br&gt;&#13;
&#13;
2. 开放词汇表检测需要模型学习一般的表示并将其与文本联系 // open-vocabulary detection requires the model to learn general representations and relate those representations to text cues. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;↪️点云领域数据收集和注释的困难 //  the difficulty of both data collection and annotation. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;↪️阻碍点云检测器学会如何将表示与文本提示连接起来 // hinders point-cloud detectors from learning to connect the representation with text prompts. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;💡Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. 提出 **`OV-3DET`** 利用**图像/视觉语言预训练模型**实现开放词汇表3D点云检测 // propose **`OV-3DET`**, which leverages advanced **`image pre-trained models`**  and **`vision-language pre-trained models`** to achieve **O**pen-**V**ocabulary **3**D point-cloud **DET**ection &#13;
2. **`OV-3DET`** 以点云和文本作为输入，并根据文本描述检测对象 不依赖于大量类标签和文本对的大规模点云数据）&#13;
&lt;div align='center'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig1.png' width='600'&gt;&#13;
 &lt;em&gt;Fig 1&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&lt;b&gt;Research Conclusion:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&#13;
&#13;
1. 提出开放词汇表的点云检测器 **`OV-3DET`**  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ✔️ 基于任意的文本描述来本地化和命名3D对象 // localize and name 3D objects based on arbitrary text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ✔️ **`OV-3DET`** 的训练不需要任何3D人工注释 // not require any 3D human annotations &lt;br&gt;&#13;
&#13;
2. 通过**二维预先训练的检测器**和**视觉语言模型实现** // **`2D image pre-trained detectors`** and **`vision-language models`**. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ✔️ 从二维预训练的检测器中定位三维对象 // localize 3D objects from **`2D pre-trained detectors`**, &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;  ✔️ 通过连接文本和点云嵌入来对检测到的对象进行分类 // classify the detected objects by connecting text and **`point-cloud embeddings`**. &lt;br&gt;&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Related Work:&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
**1. Open-Vocabulary 2D and 3D Detection**&#13;
【待总结】&#13;
&#13;
**2. Weakly-Supervised Detection**&#13;
【待总结】&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&#13;
&#13;
~~#### Framework Overview:&#13;
**`OV-3DET`** is a divide-and conquer method ➡️ two stages&#13;
① **`point-cloud detector`** learns to localize the unknown objects&#13;
② **`point-cloud detector`** learns to name them according to the text prompts.~~&#13;
&#13;
#### Method:（和·framework的序号顺序是一致的）&#13;
💡**① 【Localization 从二维预训练的检测器中获得定位能力 有展开】** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    ✔️ 直接使用二维预训练的检测器在相应的图像中生成一系列二维边界框或二维实例掩码 // directly take **`2D image pre-trained detector`** to generate a series of 2D bounding boxes or 2D instance masks in the corresponding images.  &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    ✔️ 根据点云的几何形状将棱台（取景框看fig2）转换为相对紧密的边界盒后 → 预测的二维边界盒作为点云探测器的伪边界盒 // use the **`predicted 2D bounding boxes`** as the **`pseudo bounding box`** of the point-cloud detector after transforming the **`frustum`** into relatively tight bounding box according to the **point-cloud geometry**, as shown in Fig. 2. &#13;
&#13;
&lt;div align='center' id='fig8'&gt;&#13;
    &lt;img src='https://raw.githubusercontent.com/oneHFR/xiaoxiaowu.github.io/refs/heads/main/OVD_files/img/1-fig2.png' width='500'&gt;&#13;
 &lt;em&gt;Fig 2&lt;/em&gt;&#13;
&lt;/div&gt;&#13;
&amp;nbsp;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    ✔️ 没有使用 class labels predicted by **`2D image pre-trained detector`** &lt;br&gt;&#13;
&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    ✔️ 使用粗糙的二维边界框或二维实例掩码来监督3D点云检测器来学习定位3D对象 // use the coarse 2D bounding boxes or 2D instance masks to supervise **`3D point-cloud detectors`** to learn localizing 3D objects. &lt;br&gt;&#13;
&#13;
&#13;
&#13;
💡**② 【Classification 跨模态将点云对象进行分类】** &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    ✔️ 提出一种 **去偏三重态跨模态对比学习方法** 来将点云、图像和文本联系起来 // propose a **`de-biased triplet cross-modal contrastive learning method`** to connect the modalities among point-cloud, image, and text &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;✔️ 使点云检测器能够将对象与相应的文本描述联系起来 // **`point-cloud detector`** is able to relate the objects with corresponding text descriptions. &lt;br&gt;&#13;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;✔️ 在推理过程中，只使用点云检测器和文本提示 // During inference, only **`point-cloud detector`** and  **`text prompts`** are used. &lt;br&gt;&#13;
&#13;
&amp;nbsp;&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Results:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
            &lt;p&gt;&amp;nbsp;&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='1000' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Further: (ablation study 只简介设计与操作和所得结果)&lt;/b&gt;&lt;/p&gt;&#13;
&lt;p&gt;&#13;
&#13;
 #### ablation&#13;
&#13;
&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
&lt;/table&gt;&#13;
&#13;
&#13;
### 3. Ref-paper&#13;
1. [PointCLIP: Point Cloud Understanding by CLIP](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/PointCLIP%20Point%20Cloud%20Understanding%20by%20CLIP.pdf)&#13;
&#13;
2. [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](https://github.com/oneHFR/xiaoxiaowu.github.io/blob/main/OVD_files/paper/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf)&#13;
&#13;
&amp;nbsp;&#13;
&#13;
#### 3.2 Notation and Preliminaries&#13;
$T$ ➡️ text &lt;br&gt;&#13;
$I$ ➡️ image &lt;br&gt;&#13;
$P$ ➡️ point-cloud &lt;br&gt;&#13;
$I \in \mathbb{R}^{3 \times H \times W}$, $P = \{p_i \in \mathbb{R}^3, i = 1, 2, 3, \dots, N\}$, where $N$ is the point number in the point-cloud. &lt;br&gt;&#13;
&#13;
During training, the unlabeled point-clouds dataset with its paired image is used, denotes as &lt;br&gt;&#13;
&#13;
$D^{pc}$ = ${{P_j}}^{|D_{pc}|}_{j=1}$ &#13;
&#13;
$D^{img}$  = ${I_j}^{|D_{img}|}_{j=1}$&#13;
&#13;
【还有一些没写】&lt;br&gt;&#13;
&#13;
Perform open-vocabulary classification by comparing between $f_{1D}$ (text feature) and $f_{3D}$, where $f_{3D}$ represents &#13;
&#13;
&#13;
#### 3.3 Learn to Localize 3D Objects from 2D Pre-trained Detector&#13;
1. 对于$D^{pc}$和$D^{img}$一对图像与点云 ➡️ 2D预训练探测器首先预测一系列的2D边界框或实例掩码 // For a pair of image and point-cloud from $D^{pc}$ and $D^{img}$ ➡️ 2D pre-trained detectors first predict a series of 2D bounding boxes or instance masks, if available.&#13;
&#13;
2. 将2D边界框反向投影到三维空间 ➡️ 得到3D框 // Back-project the 2D bounding box into 3D space ➡️ the frustum（棱台状）3D box that could not tightly enclose the 3D object, as shown in [Fig. 2](#fig8)&#13;
&#13;
3. 缩小三维边界框 ➡️ 利用点云的几何形状 ➡️ 对棱台内的三维点进行聚类 ➡️ 去除背景点和离群点 // Shrink the 3D bounding box ➡️ leverage the geometry of the point-cloud ➡️ perform clustering on points inside ➡️ remove background and outlier points.&#13;
&#13;
4. $L_{loc} = L_{box}^{3D}(\mathbf {\bar {b}}_{3D}, \hat {\mathbf {b}}_{3D})$,   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#13;
$\mathbf {\bar {b}}_{3D} = cluster(\mathbf {\bar {b}}_{2D} \circ K^{-1})$ &lt;br&gt;&#13;
$L_{box}^{3D}$ denotes the bounding box regression&#13;
loss used in [3DETR](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)&#13;
&#13;
&#13;
&gt; - $L_{loc}$ 表示局部损失，它依赖于三维伪边界框 $L_{box}^{3D}$ 的计算结果。</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/0917%20Reading.html</guid><pubDate>Wed, 18 Sep 2024 05:13:34 +0000</pubDate></item><item><title>Academic Reading Recording</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</link><description>&lt;table&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td valign='top' width='500'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Read Data:&lt;/b&gt; 0825&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
        &lt;td valign='top' width='500' colspan='5'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Publication:&lt;/b&gt; CVPR 2021&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;b&gt;Title:&lt;/b&gt;&#13;
            &lt;p&gt;Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing&lt;/p&gt;&#13;
            &lt;b&gt;Participants:&lt;/b&gt;&#13;
            &lt;p&gt;Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping Zhao, Quanshi Zhang&lt;/p&gt;&#13;
        &lt;/td&gt;&#13;
    &lt;/tr&gt;&#13;
    &lt;tr&gt;&#13;
        &lt;td colspan='6' valign='top' width='1000'&gt;&#13;
            &lt;p&gt;&lt;b&gt;Aim:&lt;/b&gt;&lt;/p&gt;&#13;
            &lt;p&gt;研究目标是在中间层架构和它的效用之间架起一座桥梁。</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Academic%20Reading%20Recording.html</guid><pubDate>Tue, 17 Sep 2024 04:01:58 +0000</pubDate></item><item><title>Original purpose for this blog web</title><link>https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</link><description>### BG 🤗🐑&#13;
Hey there~ I am an undergraduate student who has just switched from civil engineering to computer science. I hope to document my gradual growth process! I wish to keep writing daily summaries, continuously reflect on myself, and keep making progress!&#13;
&#13;
And right now it is 2024.09.16 I am in my junior year and literally I only know a little about python and c++ and some other basic theoretical course knowledge (will be forgotten in the short term for sure haha)&#13;
&#13;
And I don't know where I'm going to improve in the next two years so here will be my study diary ~&#13;
### Future Plan 💪💪&#13;
following years plan maybe &#13;
1. take cs degree courses; &#13;
2. figure out what academic topics I like and improve my coding skills accordingly; &#13;
3. have a satisfying summer research internship&#13;
4. I do wish have some academic publications (I know it is a wishful thinking but just a goal)&#13;
5. successfully applied for a Ph.D. degree 2026fall;。</description><guid isPermaLink="true">https://oneHFR.github.io/xiaoxiaowu.github.io/post/Original%20purpose%20for%20this%20blog%20web.html</guid><pubDate>Mon, 16 Sep 2024 12:26:43 +0000</pubDate></item></channel></rss>